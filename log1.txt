(base) asher@ashersLegion:~/CEM$ bash run_exp.sh
2025-06-18 23:59:02.496155: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-18 23:59:02.527008: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Split Learning Scheme: Overall Cutting_layer 4/13
Total number of batches per epoch for each client is  391
32
original channel size of smashed-data is 128
added bottleneck, new channel size of smashed-data is 8
local:
Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): LCALayer()
  (10): Sigmoid()
)
cloud:
Sequential(
  (0): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (10): ReLU(inplace=True)
  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (13): ReLU(inplace=True)
  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (17): ReLU(inplace=True)
  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (20): ReLU(inplace=True)
  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
classifier:
Sequential(
  (0): Dropout(p=0.5, inplace=False)
  (1): Linear(in_features=512, out_features=512, bias=True)
  (2): ReLU(inplace=True)
  (3): Dropout(p=0.5, inplace=False)
  (4): Linear(in_features=512, out_features=512, bias=True)
  (5): ReLU(inplace=True)
  (6): Linear(in_features=512, out_features=10, bias=True)
)
Namespace(arch='vgg11_bn_sgm', cutlayer=4, batch_size=128, filename='pretrain_False_lambd_0_noise_0.01_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125', folder='saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125', num_client=1, num_epochs=240, learning_rate=0.05, lambd=0.0, dataset_portion=1.0, client_sample_ratio=1.0, noniid=1.0, local_lr=-1.0, dataset='cifar10', scheme='V2_epoch', regularization='Gaussian_kl', regularization_strength=0.01, var_threshold=0.125, AT_regularization='SCA_new', AT_regularization_strength=0.3, log_entropy=1.0, ssim_threshold=0.5, gan_AE_type='res_normN4C64', gan_loss_type='SSIM', bottleneck_option='noRELU_C8S1', optimize_computation=1, decoder_sync=False, load_from_checkpoint=False, load_from_checkpoint_server=False, transfer_source_task='cifar100', finetune_freeze_bn=False, save_more_checkpoints=False, initialize_different=False, random_seed=125)
Model's smashed-data size is torch.Size([1, 8, 8, 8])
Real Train Phase: done by all clients, for total 240 epochs
GAN training interval N (once every N step) is set to 1!
Train in V2_epoch style
/home/asher/CEM/model_training_paral_pruning.py:1720: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_loss_list.append(torch.tensor(train_loss))
log--[1/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3268
log--[1/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3213
Epoch 0 Test (client-0):        Loss 2.3128 (2.3128)    Prec@1 7.812 (7.812)
Epoch 50        Test (client-0):        Loss 2.3040 (2.3041)    Prec@1 17.188 (11.458)
 * Prec@1 11.190
best model saved at: 1
train_one_ep_time:12.93771505355835 s
feature_infer_one_ep_time:2.7473790645599365 s
torch.Size([50048, 8, 8, 8])
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
/home/asher/CEM/model_training_paral_pruning.py:1909: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  rob_list.append(torch.tensor(log_det_mean.detach().cpu()))
the mean of mutal infor is:(-5.270), the est mean of mutal infor is:(-3.484)
feature_clst_one_ep_time:2.1577703952789307 s
/home/asher/anaconda3/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.
  warnings.warn(
./saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125/pretrain_False_lambd_0_noise_0.01_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125//visualize/1.png
lambd value is: 0.0 learning rate is: 0.05
/home/asher/anaconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Train in V2_epoch style
/home/asher/CEM/model_training_paral_pruning.py:1720: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_loss_list.append(torch.tensor(train_loss))
log--[2/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3343
log--[2/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.8911
Epoch 0 Test (client-0):        Loss 1.8427 (1.8427)    Prec@1 35.156 (35.156)
Epoch 50        Test (client-0):        Loss 1.7890 (1.7393)    Prec@1 35.156 (34.191)
 * Prec@1 33.580
best model saved at: 2
train_one_ep_time:12.175643682479858 s
feature_infer_one_ep_time:3.0594658851623535 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.2957286834716797 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[3/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.7915
log--[3/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.7066
Epoch 0 Test (client-0):        Loss 1.7664 (1.7664)    Prec@1 28.906 (28.906)
Epoch 50        Test (client-0):        Loss 1.7126 (1.6352)    Prec@1 32.031 (36.489)
 * Prec@1 36.460
best model saved at: 3
train_one_ep_time:12.871340036392212 s
feature_infer_one_ep_time:5.54802131652832 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.6246333122253418 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[4/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.7127
log--[4/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.5984
Epoch 0 Test (client-0):        Loss 1.4994 (1.4994)    Prec@1 37.500 (37.500)
Epoch 50        Test (client-0):        Loss 1.4299 (1.4426)    Prec@1 43.750 (45.297)
 * Prec@1 44.900
best model saved at: 4
train_one_ep_time:12.346539735794067 s
feature_infer_one_ep_time:4.132294178009033 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9270048141479492 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[5/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.4508
log--[5/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.5055
Epoch 0 Test (client-0):        Loss 1.4554 (1.4554)    Prec@1 41.406 (41.406)
Epoch 50        Test (client-0):        Loss 1.5698 (1.3971)    Prec@1 40.625 (48.683)
 * Prec@1 48.250
best model saved at: 5
train_one_ep_time:12.124612808227539 s
feature_infer_one_ep_time:2.8056869506835938 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.154599905014038 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[6/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.4083
log--[6/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.4246
Epoch 0 Test (client-0):        Loss 1.1929 (1.1929)    Prec@1 57.031 (57.031)
Epoch 50        Test (client-0):        Loss 1.2141 (1.2065)    Prec@1 53.125 (56.005)
 * Prec@1 55.510
best model saved at: 6
train_one_ep_time:14.21854567527771 s
feature_infer_one_ep_time:4.422419548034668 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0261449813842773 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[7/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.3271
log--[7/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.3414
Epoch 0 Test (client-0):        Loss 1.3080 (1.3080)    Prec@1 51.562 (51.562)
Epoch 50        Test (client-0):        Loss 1.4308 (1.2363)    Prec@1 54.688 (56.036)
 * Prec@1 55.810
best model saved at: 7
train_one_ep_time:11.370311260223389 s
feature_infer_one_ep_time:2.58327579498291 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0685582160949707 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[8/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.3638
log--[8/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.2906
Epoch 0 Test (client-0):        Loss 1.0956 (1.0956)    Prec@1 60.156 (60.156)
Epoch 50        Test (client-0):        Loss 1.1439 (1.1356)    Prec@1 64.062 (58.900)
 * Prec@1 59.240
best model saved at: 8
train_one_ep_time:11.307352542877197 s
feature_infer_one_ep_time:4.52497124671936 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8633019924163818 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[9/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.2329
log--[9/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.2408
Epoch 0 Test (client-0):        Loss 1.7245 (1.7245)    Prec@1 42.969 (42.969)
Epoch 50        Test (client-0):        Loss 1.8255 (1.5095)    Prec@1 35.938 (47.947)
 * Prec@1 47.530
train_one_ep_time:11.294743299484253 s
feature_infer_one_ep_time:2.677678346633911 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9559202194213867 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[10/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.2667
log--[10/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.1957
Epoch 0 Test (client-0):        Loss 1.0974 (1.0974)    Prec@1 54.688 (54.688)
Epoch 50        Test (client-0):        Loss 1.1173 (1.1315)    Prec@1 60.156 (59.191)
 * Prec@1 59.040
train_one_ep_time:11.329792499542236 s
feature_infer_one_ep_time:2.677990198135376 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8974244594573975 s
lambd value is: 0.0 learning rate is: 0.05
/home/asher/CEM/model_training_paral_pruning.py:1983: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  plt.figure(figsize=(10, 6))
Train in V2_epoch style
log--[11/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.2387
log--[11/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.1679
Epoch 0 Test (client-0):        Loss 1.0843 (1.0843)    Prec@1 61.719 (61.719)
Epoch 50        Test (client-0):        Loss 1.0222 (1.0998)    Prec@1 65.625 (62.148)
 * Prec@1 61.910
best model saved at: 11
train_one_ep_time:12.514210224151611 s
feature_infer_one_ep_time:2.6462695598602295 s
torch.Size([50048, 8, 8, 8])
/home/asher/CEM/model_training_paral_pruning.py:1909: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  rob_list.append(torch.tensor(log_det_mean.detach().cpu()))
the mean of mutal infor is:(-5.494), the est mean of mutal infor is:(-3.214)
feature_clst_one_ep_time:1.071136474609375 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[12/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0403
log--[12/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.1467
Epoch 0 Test (client-0):        Loss 1.0576 (1.0576)    Prec@1 64.062 (64.062)
Epoch 50        Test (client-0):        Loss 1.1591 (1.0950)    Prec@1 61.719 (62.837)
 * Prec@1 62.550
best model saved at: 12
train_one_ep_time:11.406630992889404 s
feature_infer_one_ep_time:3.239895820617676 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.2018170356750488 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[13/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0377
log--[13/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.1195
Epoch 0 Test (client-0):        Loss 0.9479 (0.9479)    Prec@1 68.750 (68.750)
Epoch 50        Test (client-0):        Loss 0.9661 (0.9707)    Prec@1 65.625 (66.682)
 * Prec@1 66.590
best model saved at: 13
train_one_ep_time:13.370424747467041 s
feature_infer_one_ep_time:2.81528902053833 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.1907083988189697 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[14/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9276
log--[14/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0968
Epoch 0 Test (client-0):        Loss 0.9796 (0.9796)    Prec@1 66.406 (66.406)
Epoch 50        Test (client-0):        Loss 1.2769 (1.1059)    Prec@1 54.688 (61.581)
 * Prec@1 61.840
train_one_ep_time:11.647734880447388 s
feature_infer_one_ep_time:2.9799132347106934 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9073152542114258 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[15/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.1174
log--[15/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0830
Epoch 0 Test (client-0):        Loss 1.0030 (1.0030)    Prec@1 67.969 (67.969)
Epoch 50        Test (client-0):        Loss 1.2252 (1.0144)    Prec@1 61.719 (64.920)
 * Prec@1 64.730
train_one_ep_time:13.561141014099121 s
feature_infer_one_ep_time:2.9659290313720703 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0762484073638916 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[16/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9167
log--[16/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0626
Epoch 0 Test (client-0):        Loss 1.2575 (1.2575)    Prec@1 57.812 (57.812)
Epoch 50        Test (client-0):        Loss 1.0925 (1.1584)    Prec@1 64.844 (59.911)
 * Prec@1 59.660
train_one_ep_time:11.496642351150513 s
feature_infer_one_ep_time:3.036208391189575 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0807678699493408 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[17/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.2609
log--[17/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0474
Epoch 0 Test (client-0):        Loss 0.9975 (0.9975)    Prec@1 64.844 (64.844)
Epoch 50        Test (client-0):        Loss 1.2286 (1.0108)    Prec@1 64.062 (65.763)
 * Prec@1 65.930
train_one_ep_time:13.096956491470337 s
feature_infer_one_ep_time:2.965743064880371 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.105926275253296 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[18/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0518
log--[18/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0415
Epoch 0 Test (client-0):        Loss 0.8569 (0.8569)    Prec@1 72.656 (72.656)
Epoch 50        Test (client-0):        Loss 0.9939 (0.9775)    Prec@1 64.062 (66.468)
 * Prec@1 66.600
best model saved at: 18
train_one_ep_time:11.806942224502563 s
feature_infer_one_ep_time:2.9344799518585205 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.885263204574585 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[19/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.1163
log--[19/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0328
Epoch 0 Test (client-0):        Loss 1.0711 (1.0711)    Prec@1 62.500 (62.500)
Epoch 50        Test (client-0):        Loss 0.9363 (0.9753)    Prec@1 70.312 (66.805)
 * Prec@1 66.600
train_one_ep_time:12.932132244110107 s
feature_infer_one_ep_time:2.800149440765381 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.090522050857544 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[20/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.1065
log--[20/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0242
Epoch 0 Test (client-0):        Loss 1.0825 (1.0825)    Prec@1 58.594 (58.594)
Epoch 50        Test (client-0):        Loss 0.9550 (1.0030)    Prec@1 68.750 (64.415)
 * Prec@1 64.860
train_one_ep_time:11.240428686141968 s
feature_infer_one_ep_time:2.971269130706787 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.1696429252624512 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[21/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0562
log--[21/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0179
Epoch 0 Test (client-0):        Loss 1.0870 (1.0870)    Prec@1 64.844 (64.844)
Epoch 50        Test (client-0):        Loss 1.1495 (1.1085)    Prec@1 64.844 (64.093)
 * Prec@1 63.980
train_one_ep_time:13.051273822784424 s
feature_infer_one_ep_time:2.6395161151885986 s
torch.Size([50048, 8, 8, 8])
the mean of mutal infor is:(-5.427), the est mean of mutal infor is:(-2.939)
feature_clst_one_ep_time:1.1397078037261963 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[22/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.1074
log--[22/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0108
Epoch 0 Test (client-0):        Loss 0.9881 (0.9881)    Prec@1 62.500 (62.500)
Epoch 50        Test (client-0):        Loss 1.3208 (1.0474)    Prec@1 53.125 (63.511)
 * Prec@1 63.130
train_one_ep_time:11.003730297088623 s
feature_infer_one_ep_time:2.6678426265716553 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0307493209838867 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[23/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9897
log--[23/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0048
Epoch 0 Test (client-0):        Loss 0.9028 (0.9028)    Prec@1 67.969 (67.969)
Epoch 50        Test (client-0):        Loss 0.8900 (0.9271)    Prec@1 71.094 (68.704)
 * Prec@1 68.460
best model saved at: 23
train_one_ep_time:13.274940729141235 s
feature_infer_one_ep_time:2.611586809158325 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.1099481582641602 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[24/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9993
log--[24/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0069
Epoch 0 Test (client-0):        Loss 0.8854 (0.8854)    Prec@1 71.094 (71.094)
Epoch 50        Test (client-0):        Loss 1.0416 (0.9010)    Prec@1 71.094 (68.612)
 * Prec@1 68.650
best model saved at: 24
train_one_ep_time:11.109969139099121 s
feature_infer_one_ep_time:2.6137492656707764 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9864592552185059 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[25/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.8974
log--[25/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0019
Epoch 0 Test (client-0):        Loss 1.0449 (1.0449)    Prec@1 60.938 (60.938)
Epoch 50        Test (client-0):        Loss 1.0594 (1.0020)    Prec@1 67.188 (65.947)
 * Prec@1 65.870
train_one_ep_time:13.083354234695435 s
feature_infer_one_ep_time:2.7461631298065186 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.916841983795166 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[26/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9905
log--[26/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9894
Epoch 0 Test (client-0):        Loss 0.8463 (0.8463)    Prec@1 68.750 (68.750)
Epoch 50        Test (client-0):        Loss 1.0310 (0.9354)    Prec@1 67.188 (67.341)
 * Prec@1 67.580
train_one_ep_time:11.134375095367432 s
feature_infer_one_ep_time:2.7706210613250732 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0069270133972168 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[27/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0959
log--[27/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9842
Epoch 0 Test (client-0):        Loss 0.9524 (0.9524)    Prec@1 64.844 (64.844)
Epoch 50        Test (client-0):        Loss 1.1348 (1.0347)    Prec@1 61.719 (65.579)
 * Prec@1 65.340
train_one_ep_time:11.057032108306885 s
feature_infer_one_ep_time:2.8588924407958984 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0349960327148438 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[28/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.8856
log--[28/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9833
Epoch 0 Test (client-0):        Loss 1.2481 (1.2481)    Prec@1 56.250 (56.250)
Epoch 50        Test (client-0):        Loss 1.1545 (1.1228)    Prec@1 61.719 (62.929)
 * Prec@1 63.380
train_one_ep_time:11.17283034324646 s
feature_infer_one_ep_time:4.9352569580078125 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0475835800170898 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[29/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9761
log--[29/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9878
Epoch 0 Test (client-0):        Loss 0.8590 (0.8590)    Prec@1 68.750 (68.750)
Epoch 50        Test (client-0):        Loss 0.9539 (0.9017)    Prec@1 68.750 (68.919)
 * Prec@1 68.960
best model saved at: 29
train_one_ep_time:10.941300630569458 s
feature_infer_one_ep_time:2.66092848777771 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.2025086879730225 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[30/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9131
log--[30/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9687
Epoch 0 Test (client-0):        Loss 0.8788 (0.8788)    Prec@1 72.656 (72.656)
Epoch 50        Test (client-0):        Loss 0.8419 (0.9341)    Prec@1 71.875 (68.490)
 * Prec@1 68.550
train_one_ep_time:11.38900089263916 s
feature_infer_one_ep_time:2.9124855995178223 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:2.799651861190796 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[31/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0201
log--[31/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9770
Epoch 0 Test (client-0):        Loss 0.9276 (0.9276)    Prec@1 71.875 (71.875)
Epoch 50        Test (client-0):        Loss 1.0618 (0.8848)    Prec@1 61.719 (68.873)
 * Prec@1 69.080
best model saved at: 31
train_one_ep_time:11.23835802078247 s
feature_infer_one_ep_time:2.784142255783081 s
torch.Size([50048, 8, 8, 8])
the mean of mutal infor is:(-5.736), the est mean of mutal infor is:(-3.230)
feature_clst_one_ep_time:1.1713175773620605 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[32/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.7274
log--[32/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9622
Epoch 0 Test (client-0):        Loss 1.1732 (1.1732)    Prec@1 61.719 (61.719)
Epoch 50        Test (client-0):        Loss 1.1218 (1.1238)    Prec@1 60.938 (63.373)
 * Prec@1 63.580
train_one_ep_time:11.330045938491821 s
feature_infer_one_ep_time:2.9121205806732178 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.2552478313446045 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[33/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.8926
log--[33/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9599
Epoch 0 Test (client-0):        Loss 1.0209 (1.0209)    Prec@1 64.844 (64.844)
Epoch 50        Test (client-0):        Loss 0.9669 (0.9650)    Prec@1 67.188 (66.284)
 * Prec@1 66.140
train_one_ep_time:13.415597677230835 s
feature_infer_one_ep_time:3.045461416244507 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0894279479980469 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[34/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9639
log--[34/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9544
Epoch 0 Test (client-0):        Loss 0.7636 (0.7636)    Prec@1 75.000 (75.000)
Epoch 50        Test (client-0):        Loss 0.8014 (0.8829)    Prec@1 72.656 (70.343)
 * Prec@1 70.210
best model saved at: 34
train_one_ep_time:11.71661901473999 s
feature_infer_one_ep_time:2.9911117553710938 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.1797595024108887 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[35/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9630
log--[35/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9540
Epoch 0 Test (client-0):        Loss 0.9959 (0.9959)    Prec@1 66.406 (66.406)
Epoch 50        Test (client-0):        Loss 0.8984 (0.9453)    Prec@1 71.094 (68.888)
 * Prec@1 68.660
train_one_ep_time:13.27879023551941 s
feature_infer_one_ep_time:2.7145328521728516 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0189971923828125 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[36/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0166
log--[36/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9518
Epoch 0 Test (client-0):        Loss 0.8704 (0.8704)    Prec@1 67.969 (67.969)
Epoch 50        Test (client-0):        Loss 0.9261 (0.9757)    Prec@1 67.188 (66.284)
 * Prec@1 66.460
train_one_ep_time:11.378755331039429 s
feature_infer_one_ep_time:2.690263032913208 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.028789758682251 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[37/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0902
log--[37/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9456
Epoch 0 Test (client-0):        Loss 0.8262 (0.8262)    Prec@1 75.000 (75.000)
Epoch 50        Test (client-0):        Loss 0.8811 (0.8364)    Prec@1 72.656 (71.385)
 * Prec@1 71.140
best model saved at: 37
train_one_ep_time:13.000298738479614 s
feature_infer_one_ep_time:2.6370508670806885 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.033935785293579 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[38/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9768
log--[38/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9388
Epoch 0 Test (client-0):        Loss 0.8983 (0.8983)    Prec@1 71.094 (71.094)
Epoch 50        Test (client-0):        Loss 1.0888 (0.9785)    Prec@1 62.500 (68.107)
 * Prec@1 67.870
train_one_ep_time:11.359477043151855 s
feature_infer_one_ep_time:2.861638307571411 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8967914581298828 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[39/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.8033
log--[39/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9469
Epoch 0 Test (client-0):        Loss 1.0509 (1.0509)    Prec@1 62.500 (62.500)
Epoch 50        Test (client-0):        Loss 1.1663 (1.0219)    Prec@1 65.625 (65.411)
 * Prec@1 64.860
train_one_ep_time:12.979292631149292 s
feature_infer_one_ep_time:2.8218350410461426 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8924057483673096 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[40/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9241
log--[40/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9449
Epoch 0 Test (client-0):        Loss 1.1970 (1.1970)    Prec@1 59.375 (59.375)
Epoch 50        Test (client-0):        Loss 1.3131 (1.2252)    Prec@1 57.812 (59.161)
 * Prec@1 59.080
train_one_ep_time:11.029314994812012 s
feature_infer_one_ep_time:2.8726813793182373 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9969673156738281 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[41/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.2115
log--[41/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9447
Epoch 0 Test (client-0):        Loss 0.9723 (0.9723)    Prec@1 64.062 (64.062)
Epoch 50        Test (client-0):        Loss 1.0701 (0.9709)    Prec@1 68.750 (67.647)
 * Prec@1 67.690
best model saved at: 41
train_one_ep_time:13.231697082519531 s
feature_infer_one_ep_time:3.253875970840454 s
torch.Size([50048, 8, 8, 8])
the mean of mutal infor is:(-5.470), the est mean of mutal infor is:(-2.964)
feature_clst_one_ep_time:1.3136775493621826 s
/home/asher/anaconda3/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.
  warnings.warn(
./saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125/pretrain_False_lambd_0_noise_0.01_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125//visualize/41.png
lambd value is: 0.0 learning rate is: 0.05
/home/asher/anaconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Train in V2_epoch style
/home/asher/CEM/model_training_paral_pruning.py:1720: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_loss_list.append(torch.tensor(train_loss))
log--[42/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.8816
log--[42/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9306
Epoch 0 Test (client-0):        Loss 0.8665 (0.8665)    Prec@1 69.531 (69.531)
Epoch 50        Test (client-0):        Loss 0.8398 (0.8375)    Prec@1 71.094 (71.798)
 * Prec@1 71.810
best model saved at: 42
train_one_ep_time:13.497767925262451 s
feature_infer_one_ep_time:3.1752147674560547 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0040490627288818 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[43/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.7426
log--[43/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9315
Epoch 0 Test (client-0):        Loss 0.7958 (0.7958)    Prec@1 78.125 (78.125)
Epoch 50        Test (client-0):        Loss 0.9060 (0.8242)    Prec@1 74.219 (72.687)
 * Prec@1 72.470
best model saved at: 43
train_one_ep_time:11.325345277786255 s
feature_infer_one_ep_time:2.925903558731079 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.025240421295166 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[44/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9552
log--[44/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9283
Epoch 0 Test (client-0):        Loss 0.9871 (0.9871)    Prec@1 64.062 (64.062)
Epoch 50        Test (client-0):        Loss 0.9478 (0.9363)    Prec@1 69.531 (67.601)
 * Prec@1 67.310
train_one_ep_time:14.13360071182251 s
feature_infer_one_ep_time:3.7114884853363037 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0801184177398682 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[45/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.8748
log--[45/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9280
Epoch 0 Test (client-0):        Loss 0.9819 (0.9819)    Prec@1 60.938 (60.938)
Epoch 50        Test (client-0):        Loss 0.9225 (0.8774)    Prec@1 66.406 (69.378)
 * Prec@1 69.490
train_one_ep_time:11.64218282699585 s
feature_infer_one_ep_time:3.0140364170074463 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.100431203842163 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[46/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.1649
log--[46/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9263
Epoch 0 Test (client-0):        Loss 0.8110 (0.8110)    Prec@1 71.094 (71.094)
Epoch 50        Test (client-0):        Loss 0.8529 (0.8474)    Prec@1 71.094 (70.987)
 * Prec@1 70.750
train_one_ep_time:13.007197856903076 s
feature_infer_one_ep_time:2.9002010822296143 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.1376523971557617 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[47/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9565
log--[47/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9349
Epoch 0 Test (client-0):        Loss 0.8623 (0.8623)    Prec@1 72.656 (72.656)
Epoch 50        Test (client-0):        Loss 0.8188 (0.8400)    Prec@1 76.562 (71.676)
 * Prec@1 71.740
train_one_ep_time:11.49832534790039 s
feature_infer_one_ep_time:4.142430782318115 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.089920997619629 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[48/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9739
log--[48/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9273
Epoch 0 Test (client-0):        Loss 0.8443 (0.8443)    Prec@1 69.531 (69.531)
Epoch 50        Test (client-0):        Loss 0.9846 (0.9117)    Prec@1 69.531 (70.159)
 * Prec@1 69.950
train_one_ep_time:15.004806518554688 s
feature_infer_one_ep_time:3.139920234680176 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0377626419067383 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[49/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9252
log--[49/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9209
Epoch 0 Test (client-0):        Loss 1.0495 (1.0495)    Prec@1 64.062 (64.062)
Epoch 50        Test (client-0):        Loss 0.9060 (0.9439)    Prec@1 71.875 (68.061)
 * Prec@1 68.350
train_one_ep_time:11.875517129898071 s
feature_infer_one_ep_time:3.0232303142547607 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.976182222366333 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[50/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0657
log--[50/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9176
Epoch 0 Test (client-0):        Loss 0.9196 (0.9196)    Prec@1 71.094 (71.094)
Epoch 50        Test (client-0):        Loss 0.9174 (0.9240)    Prec@1 65.625 (69.301)
 * Prec@1 68.800
train_one_ep_time:14.429060697555542 s
feature_infer_one_ep_time:3.187628984451294 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9379374980926514 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[51/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0213
log--[51/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9288
Epoch 0 Test (client-0):        Loss 0.8849 (0.8849)    Prec@1 72.656 (72.656)
Epoch 50        Test (client-0):        Loss 0.8518 (0.9054)    Prec@1 73.438 (69.899)
 * Prec@1 69.640
train_one_ep_time:12.03143048286438 s
feature_infer_one_ep_time:3.0403356552124023 s
torch.Size([50048, 8, 8, 8])
/home/asher/CEM/model_training_paral_pruning.py:1909: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  rob_list.append(torch.tensor(log_det_mean.detach().cpu()))
the mean of mutal infor is:(-5.437), the est mean of mutal infor is:(-3.052)
feature_clst_one_ep_time:1.0406763553619385 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[52/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.8882
log--[52/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9225
Epoch 0 Test (client-0):        Loss 0.8359 (0.8359)    Prec@1 73.438 (73.438)
Epoch 50        Test (client-0):        Loss 0.9221 (0.8759)    Prec@1 68.750 (70.665)
 * Prec@1 70.550
train_one_ep_time:14.147157430648804 s
feature_infer_one_ep_time:2.952559471130371 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.1972837448120117 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[53/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9066
log--[53/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9162
Epoch 0 Test (client-0):        Loss 1.0671 (1.0671)    Prec@1 67.969 (67.969)
Epoch 50        Test (client-0):        Loss 1.0821 (1.1236)    Prec@1 64.844 (64.798)
 * Prec@1 64.720
train_one_ep_time:11.783581256866455 s
feature_infer_one_ep_time:2.8077807426452637 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.1111438274383545 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[54/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.8386
log--[54/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9166
Epoch 0 Test (client-0):        Loss 0.8662 (0.8662)    Prec@1 71.875 (71.875)
Epoch 50        Test (client-0):        Loss 0.9050 (0.8804)    Prec@1 73.438 (70.159)
 * Prec@1 70.480
train_one_ep_time:13.154282093048096 s
feature_infer_one_ep_time:3.3434908390045166 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.1185741424560547 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[55/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9653
log--[55/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9008
Epoch 0 Test (client-0):        Loss 1.1192 (1.1192)    Prec@1 60.938 (60.938)
Epoch 50        Test (client-0):        Loss 1.0285 (1.0226)    Prec@1 63.281 (64.859)
 * Prec@1 65.000
train_one_ep_time:13.89761996269226 s
feature_infer_one_ep_time:2.860473394393921 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9879155158996582 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[56/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 1.0144
log--[56/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9151
Epoch 0 Test (client-0):        Loss 0.8474 (0.8474)    Prec@1 74.219 (74.219)
Epoch 50        Test (client-0):        Loss 0.8003 (0.8234)    Prec@1 75.781 (72.534)
 * Prec@1 72.090
train_one_ep_time:11.784928798675537 s
feature_infer_one_ep_time:2.945937156677246 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9765756130218506 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[57/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9342
log--[57/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9154
Epoch 0 Test (client-0):        Loss 0.7504 (0.7504)    Prec@1 76.562 (76.562)
Epoch 50        Test (client-0):        Loss 0.7734 (0.8243)    Prec@1 74.219 (72.381)
 * Prec@1 72.470
train_one_ep_time:13.627833843231201 s
feature_infer_one_ep_time:2.8156533241271973 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0834143161773682 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[58/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9471
log--[58/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9149
Epoch 0 Test (client-0):        Loss 0.8503 (0.8503)    Prec@1 71.875 (71.875)
Epoch 50        Test (client-0):        Loss 0.9332 (0.8822)    Prec@1 67.969 (68.903)
 * Prec@1 68.670
train_one_ep_time:11.823773384094238 s
feature_infer_one_ep_time:3.0713818073272705 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0956108570098877 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[59/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9386
log--[59/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.9166
Epoch 0 Test (client-0):        Loss 0.8120 (0.8120)    Prec@1 74.219 (74.219)
Epoch 50        Test (client-0):        Loss 0.9112 (0.8704)    Prec@1 71.875 (70.251)
 * Prec@1 70.470
train_one_ep_time:14.206680297851562 s
feature_infer_one_ep_time:3.000101327896118 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0723650455474854 s
lambd value is: 0.0 learning rate is: 0.05
Train in V2_epoch style
log--[60/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.7945
log--[60/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.7302
Epoch 0 Test (client-0):        Loss 0.6330 (0.6330)    Prec@1 78.906 (78.906)
Epoch 50        Test (client-0):        Loss 0.6689 (0.6310)    Prec@1 79.688 (78.523)
 * Prec@1 78.800
best model saved at: 60
train_one_ep_time:11.834661483764648 s
feature_infer_one_ep_time:2.9883828163146973 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0410375595092773 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[61/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.8272
log--[61/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6915
Epoch 0 Test (client-0):        Loss 0.5320 (0.5320)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.6092 (0.5879)    Prec@1 83.594 (80.055)
 * Prec@1 80.120
best model saved at: 61
train_one_ep_time:14.001017332077026 s
feature_infer_one_ep_time:3.1291637420654297 s
torch.Size([50048, 8, 8, 8])
the mean of mutal infor is:(-6.065), the est mean of mutal infor is:(-3.120)
feature_clst_one_ep_time:0.9092941284179688 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[62/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.7323
log--[62/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6693
Epoch 0 Test (client-0):        Loss 0.5992 (0.5992)    Prec@1 80.469 (80.469)
Epoch 50        Test (client-0):        Loss 0.5415 (0.5883)    Prec@1 84.375 (79.611)
 * Prec@1 79.790
train_one_ep_time:12.068733930587769 s
feature_infer_one_ep_time:3.1445765495300293 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8668866157531738 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[63/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.8280
log--[63/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6720
Epoch 0 Test (client-0):        Loss 0.6156 (0.6156)    Prec@1 79.688 (79.688)
Epoch 50        Test (client-0):        Loss 0.5443 (0.6013)    Prec@1 82.031 (79.519)
 * Prec@1 79.800
train_one_ep_time:13.92090916633606 s
feature_infer_one_ep_time:3.4942004680633545 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8711037635803223 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[64/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6454
log--[64/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6531
Epoch 0 Test (client-0):        Loss 0.6588 (0.6588)    Prec@1 78.125 (78.125)
Epoch 50        Test (client-0):        Loss 0.6463 (0.6070)    Prec@1 80.469 (79.136)
 * Prec@1 79.340
train_one_ep_time:12.139908790588379 s
feature_infer_one_ep_time:3.4294517040252686 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:2.507491111755371 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[65/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.7331
log--[65/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6535
Epoch 0 Test (client-0):        Loss 0.6738 (0.6738)    Prec@1 78.125 (78.125)
Epoch 50        Test (client-0):        Loss 0.6913 (0.6399)    Prec@1 78.125 (78.079)
 * Prec@1 78.120
train_one_ep_time:12.056660890579224 s
feature_infer_one_ep_time:3.148505687713623 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8999087810516357 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[66/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5603
log--[66/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6486
Epoch 0 Test (client-0):        Loss 0.6577 (0.6577)    Prec@1 77.344 (77.344)
Epoch 50        Test (client-0):        Loss 0.5904 (0.6324)    Prec@1 85.938 (78.600)
 * Prec@1 78.400
train_one_ep_time:12.369110107421875 s
feature_infer_one_ep_time:4.8860087394714355 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9265344142913818 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[67/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.7799
log--[67/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6491
Epoch 0 Test (client-0):        Loss 0.5890 (0.5890)    Prec@1 78.125 (78.125)
Epoch 50        Test (client-0):        Loss 0.6520 (0.6147)    Prec@1 79.688 (78.554)
 * Prec@1 78.970
train_one_ep_time:12.279803991317749 s
feature_infer_one_ep_time:3.09377384185791 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8542819023132324 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[68/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6712
log--[68/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6466
Epoch 0 Test (client-0):        Loss 0.5967 (0.5967)    Prec@1 82.812 (82.812)
Epoch 50        Test (client-0):        Loss 0.5711 (0.5899)    Prec@1 82.812 (80.147)
 * Prec@1 80.110
train_one_ep_time:12.388064861297607 s
feature_infer_one_ep_time:4.84289288520813 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8318231105804443 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[69/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6294
log--[69/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6449
Epoch 0 Test (client-0):        Loss 0.6175 (0.6175)    Prec@1 81.250 (81.250)
Epoch 50        Test (client-0):        Loss 0.6011 (0.6065)    Prec@1 80.469 (79.994)
 * Prec@1 79.980
train_one_ep_time:11.809407234191895 s
feature_infer_one_ep_time:3.0221681594848633 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0074613094329834 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[70/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5861
log--[70/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6400
Epoch 0 Test (client-0):        Loss 0.6855 (0.6855)    Prec@1 78.906 (78.906)
Epoch 50        Test (client-0):        Loss 0.5545 (0.5802)    Prec@1 81.250 (80.009)
 * Prec@1 80.260
best model saved at: 70
train_one_ep_time:12.420931100845337 s
feature_infer_one_ep_time:4.517603158950806 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9565191268920898 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[71/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6330
log--[71/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6318
Epoch 0 Test (client-0):        Loss 0.6719 (0.6719)    Prec@1 79.688 (79.688)
Epoch 50        Test (client-0):        Loss 0.5308 (0.6042)    Prec@1 85.938 (79.933)
 * Prec@1 79.940
train_one_ep_time:12.059224843978882 s
feature_infer_one_ep_time:3.0390021800994873 s
torch.Size([50048, 8, 8, 8])
the mean of mutal infor is:(-7.131), the est mean of mutal infor is:(-3.550)
feature_clst_one_ep_time:0.9990055561065674 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[72/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6722
log--[72/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6355
Epoch 0 Test (client-0):        Loss 0.6241 (0.6241)    Prec@1 80.469 (80.469)
Epoch 50        Test (client-0):        Loss 0.6230 (0.6116)    Prec@1 80.469 (79.259)
 * Prec@1 79.240
train_one_ep_time:11.939086437225342 s
feature_infer_one_ep_time:4.820709943771362 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.80120849609375 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[73/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6914
log--[73/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6439
Epoch 0 Test (client-0):        Loss 0.6577 (0.6577)    Prec@1 78.906 (78.906)
Epoch 50        Test (client-0):        Loss 0.7420 (0.6351)    Prec@1 78.906 (78.738)
 * Prec@1 78.770
train_one_ep_time:11.699352741241455 s
feature_infer_one_ep_time:2.988079071044922 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6913089752197266 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[74/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5787
log--[74/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6354
Epoch 0 Test (client-0):        Loss 0.7005 (0.7005)    Prec@1 78.125 (78.125)
Epoch 50        Test (client-0):        Loss 0.7695 (0.6886)    Prec@1 79.688 (76.930)
 * Prec@1 76.910
train_one_ep_time:12.284152507781982 s
feature_infer_one_ep_time:4.840275526046753 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0072784423828125 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[75/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5635
log--[75/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6319
Epoch 0 Test (client-0):        Loss 0.6205 (0.6205)    Prec@1 77.344 (77.344)
Epoch 50        Test (client-0):        Loss 0.6507 (0.6359)    Prec@1 78.906 (78.324)
 * Prec@1 78.710
train_one_ep_time:11.888227939605713 s
feature_infer_one_ep_time:2.96628475189209 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7873249053955078 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[76/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.7308
log--[76/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6317
Epoch 0 Test (client-0):        Loss 0.6307 (0.6307)    Prec@1 78.906 (78.906)
Epoch 50        Test (client-0):        Loss 0.5860 (0.5856)    Prec@1 82.812 (79.902)
 * Prec@1 80.220
train_one_ep_time:12.211772680282593 s
feature_infer_one_ep_time:4.9952991008758545 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9695525169372559 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[77/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5971
log--[77/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6300
Epoch 0 Test (client-0):        Loss 0.7169 (0.7169)    Prec@1 72.656 (72.656)
Epoch 50        Test (client-0):        Loss 0.6857 (0.6323)    Prec@1 80.469 (78.508)
 * Prec@1 78.640
train_one_ep_time:12.158548593521118 s
feature_infer_one_ep_time:3.034550905227661 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9722356796264648 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[78/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6248
log--[78/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6346
Epoch 0 Test (client-0):        Loss 0.6617 (0.6617)    Prec@1 78.125 (78.125)
Epoch 50        Test (client-0):        Loss 0.7258 (0.6143)    Prec@1 77.344 (78.784)
 * Prec@1 79.020
train_one_ep_time:12.28571081161499 s
feature_infer_one_ep_time:5.0444488525390625 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8805437088012695 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[79/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5302
log--[79/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6302
Epoch 0 Test (client-0):        Loss 0.5970 (0.5970)    Prec@1 80.469 (80.469)
Epoch 50        Test (client-0):        Loss 0.6981 (0.5831)    Prec@1 79.688 (80.040)
 * Prec@1 80.010
train_one_ep_time:11.96870732307434 s
feature_infer_one_ep_time:7.685128688812256 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.5691289901733398 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[80/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6736
log--[80/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6333
Epoch 0 Test (client-0):        Loss 0.6122 (0.6122)    Prec@1 80.469 (80.469)
Epoch 50        Test (client-0):        Loss 0.5849 (0.5516)    Prec@1 84.375 (81.373)
 * Prec@1 81.530
best model saved at: 80
train_one_ep_time:14.594320297241211 s
feature_infer_one_ep_time:2.98803973197937 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8410987854003906 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[81/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6026
log--[81/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6282
Epoch 0 Test (client-0):        Loss 0.6376 (0.6376)    Prec@1 78.125 (78.125)
Epoch 50        Test (client-0):        Loss 0.7116 (0.5878)    Prec@1 78.906 (79.718)
 * Prec@1 80.030
train_one_ep_time:12.626964807510376 s
feature_infer_one_ep_time:3.0809121131896973 s
torch.Size([50048, 8, 8, 8])
the mean of mutal infor is:(-7.546), the est mean of mutal infor is:(-3.817)
feature_clst_one_ep_time:0.8419113159179688 s
/home/asher/anaconda3/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.
  warnings.warn(
./saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125/pretrain_False_lambd_0_noise_0.01_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125//visualize/81.png
lambd value is: 0.0 learning rate is: 0.010000000000000002
/home/asher/anaconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Train in V2_epoch style
/home/asher/CEM/model_training_paral_pruning.py:1720: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_loss_list.append(torch.tensor(train_loss))
log--[82/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6182
log--[82/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6274
Epoch 0 Test (client-0):        Loss 0.6200 (0.6200)    Prec@1 76.562 (76.562)
Epoch 50        Test (client-0):        Loss 0.6212 (0.6340)    Prec@1 78.125 (78.753)
 * Prec@1 78.820
train_one_ep_time:12.376530647277832 s
feature_infer_one_ep_time:3.1066625118255615 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0302526950836182 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[83/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6270
log--[83/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6221
Epoch 0 Test (client-0):        Loss 0.5737 (0.5737)    Prec@1 80.469 (80.469)
Epoch 50        Test (client-0):        Loss 0.6346 (0.6025)    Prec@1 80.469 (79.243)
 * Prec@1 79.280
train_one_ep_time:14.098223209381104 s
feature_infer_one_ep_time:2.9847607612609863 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.1264383792877197 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[84/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4859
log--[84/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6321
Epoch 0 Test (client-0):        Loss 0.6557 (0.6557)    Prec@1 77.344 (77.344)
Epoch 50        Test (client-0):        Loss 0.7074 (0.6037)    Prec@1 77.344 (79.381)
 * Prec@1 79.330
train_one_ep_time:12.3750638961792 s
feature_infer_one_ep_time:3.049929141998291 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9501218795776367 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[85/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4850
log--[85/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6202
Epoch 0 Test (client-0):        Loss 0.5716 (0.5716)    Prec@1 84.375 (84.375)
Epoch 50        Test (client-0):        Loss 0.6187 (0.6133)    Prec@1 83.594 (79.427)
 * Prec@1 79.380
train_one_ep_time:13.924288272857666 s
feature_infer_one_ep_time:3.2389538288116455 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9973537921905518 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[86/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6549
log--[86/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6277
Epoch 0 Test (client-0):        Loss 0.5824 (0.5824)    Prec@1 82.812 (82.812)
Epoch 50        Test (client-0):        Loss 0.5887 (0.5803)    Prec@1 80.469 (80.132)
 * Prec@1 80.330
train_one_ep_time:12.613312482833862 s
feature_infer_one_ep_time:3.4369359016418457 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9087538719177246 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[87/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4556
log--[87/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6222
Epoch 0 Test (client-0):        Loss 0.5946 (0.5946)    Prec@1 78.125 (78.125)
Epoch 50        Test (client-0):        Loss 0.6133 (0.5742)    Prec@1 82.031 (80.270)
 * Prec@1 80.470
train_one_ep_time:12.400736570358276 s
feature_infer_one_ep_time:3.16336989402771 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.977311372756958 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[88/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6426
log--[88/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6180
Epoch 0 Test (client-0):        Loss 0.5978 (0.5978)    Prec@1 82.031 (82.031)
Epoch 50        Test (client-0):        Loss 0.6699 (0.6364)    Prec@1 79.688 (78.263)
 * Prec@1 78.690
train_one_ep_time:15.252417802810669 s
feature_infer_one_ep_time:3.218669891357422 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9774656295776367 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[89/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5321
log--[89/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6201
Epoch 0 Test (client-0):        Loss 0.6374 (0.6374)    Prec@1 76.562 (76.562)
Epoch 50        Test (client-0):        Loss 0.5872 (0.5814)    Prec@1 84.375 (80.116)
 * Prec@1 80.220
train_one_ep_time:12.418981313705444 s
feature_infer_one_ep_time:3.333240509033203 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0563368797302246 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[90/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4487
log--[90/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6179
Epoch 0 Test (client-0):        Loss 0.6694 (0.6694)    Prec@1 77.344 (77.344)
Epoch 50        Test (client-0):        Loss 0.7287 (0.6545)    Prec@1 75.000 (77.298)
 * Prec@1 77.590
train_one_ep_time:14.391216039657593 s
feature_infer_one_ep_time:3.0994584560394287 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0232255458831787 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[91/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6698
log--[91/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6151
Epoch 0 Test (client-0):        Loss 0.6294 (0.6294)    Prec@1 80.469 (80.469)
Epoch 50        Test (client-0):        Loss 0.5921 (0.5967)    Prec@1 82.812 (80.070)
 * Prec@1 80.360
train_one_ep_time:12.462143898010254 s
feature_infer_one_ep_time:3.101949691772461 s
torch.Size([50048, 8, 8, 8])
/home/asher/CEM/model_training_paral_pruning.py:1909: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  rob_list.append(torch.tensor(log_det_mean.detach().cpu()))
the mean of mutal infor is:(-7.790), the est mean of mutal infor is:(-3.923)
feature_clst_one_ep_time:2.9707818031311035 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[92/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5176
log--[92/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6138
Epoch 0 Test (client-0):        Loss 0.7744 (0.7744)    Prec@1 75.781 (75.781)
Epoch 50        Test (client-0):        Loss 0.6909 (0.6771)    Prec@1 77.344 (77.619)
 * Prec@1 77.390
train_one_ep_time:12.355297803878784 s
feature_infer_one_ep_time:3.0629281997680664 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9233286380767822 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[93/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5106
log--[93/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6180
Epoch 0 Test (client-0):        Loss 0.5750 (0.5750)    Prec@1 82.031 (82.031)
Epoch 50        Test (client-0):        Loss 0.7196 (0.6178)    Prec@1 77.344 (79.396)
 * Prec@1 79.450
train_one_ep_time:12.134873867034912 s
feature_infer_one_ep_time:4.9213035106658936 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.006998062133789 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[94/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5480
log--[94/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6113
Epoch 0 Test (client-0):        Loss 0.5578 (0.5578)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.6111 (0.5882)    Prec@1 79.688 (80.040)
 * Prec@1 80.140
train_one_ep_time:12.436840295791626 s
feature_infer_one_ep_time:2.9732282161712646 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9635016918182373 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[95/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5175
log--[95/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6155
Epoch 0 Test (client-0):        Loss 0.6037 (0.6037)    Prec@1 79.688 (79.688)
Epoch 50        Test (client-0):        Loss 0.6079 (0.5974)    Prec@1 81.250 (79.688)
 * Prec@1 79.460
train_one_ep_time:12.34395146369934 s
feature_infer_one_ep_time:4.905769109725952 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0533018112182617 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[96/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5096
log--[96/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6136
Epoch 0 Test (client-0):        Loss 0.5525 (0.5525)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.5985 (0.5682)    Prec@1 81.250 (80.545)
 * Prec@1 80.680
train_one_ep_time:12.50310730934143 s
feature_infer_one_ep_time:3.022573232650757 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8600099086761475 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[97/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5818
log--[97/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6019
Epoch 0 Test (client-0):        Loss 0.5759 (0.5759)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.6180 (0.5476)    Prec@1 81.250 (81.219)
 * Prec@1 81.150
train_one_ep_time:12.156659841537476 s
feature_infer_one_ep_time:4.875143527984619 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9734265804290771 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[98/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5241
log--[98/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5986
Epoch 0 Test (client-0):        Loss 0.7856 (0.7856)    Prec@1 74.219 (74.219)
Epoch 50        Test (client-0):        Loss 0.8084 (0.6902)    Prec@1 78.125 (77.451)
 * Prec@1 77.490
train_one_ep_time:12.28273320198059 s
feature_infer_one_ep_time:3.3889710903167725 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.857630729675293 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[99/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5705
log--[99/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6135
Epoch 0 Test (client-0):        Loss 0.7283 (0.7283)    Prec@1 77.344 (77.344)
Epoch 50        Test (client-0):        Loss 0.7761 (0.6261)    Prec@1 78.125 (78.968)
 * Prec@1 79.080
train_one_ep_time:12.545843362808228 s
feature_infer_one_ep_time:5.116556406021118 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.02065110206604 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[100/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6850
log--[100/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6043
Epoch 0 Test (client-0):        Loss 0.6477 (0.6477)    Prec@1 82.031 (82.031)
Epoch 50        Test (client-0):        Loss 0.6021 (0.6190)    Prec@1 80.469 (79.289)
 * Prec@1 78.950
train_one_ep_time:12.645414590835571 s
feature_infer_one_ep_time:3.1484534740448 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8980376720428467 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[101/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4826
log--[101/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5984
Epoch 0 Test (client-0):        Loss 0.6230 (0.6230)    Prec@1 78.906 (78.906)
Epoch 50        Test (client-0):        Loss 0.6126 (0.5797)    Prec@1 80.469 (80.699)
 * Prec@1 80.490
best model saved at: 101
train_one_ep_time:13.903966903686523 s
feature_infer_one_ep_time:3.2078559398651123 s
torch.Size([50048, 8, 8, 8])
the mean of mutal infor is:(-7.971), the est mean of mutal infor is:(-4.071)
feature_clst_one_ep_time:1.3950152397155762 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[102/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.7787
log--[102/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6022
Epoch 0 Test (client-0):        Loss 0.6036 (0.6036)    Prec@1 78.125 (78.125)
Epoch 50        Test (client-0):        Loss 0.6116 (0.5768)    Prec@1 82.812 (79.994)
 * Prec@1 80.300
train_one_ep_time:13.373873949050903 s
feature_infer_one_ep_time:3.1756303310394287 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.974860668182373 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[103/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6577
log--[103/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6004
Epoch 0 Test (client-0):        Loss 0.5504 (0.5504)    Prec@1 82.031 (82.031)
Epoch 50        Test (client-0):        Loss 0.6222 (0.5714)    Prec@1 81.250 (80.683)
 * Prec@1 80.760
best model saved at: 103
train_one_ep_time:13.241222381591797 s
feature_infer_one_ep_time:3.0278427600860596 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.783940315246582 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[104/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6382
log--[104/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6007
Epoch 0 Test (client-0):        Loss 0.6856 (0.6856)    Prec@1 80.469 (80.469)
Epoch 50        Test (client-0):        Loss 0.5731 (0.5697)    Prec@1 82.031 (80.545)
 * Prec@1 80.750
train_one_ep_time:11.785672187805176 s
feature_infer_one_ep_time:2.91422700881958 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9650447368621826 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[105/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5501
log--[105/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5944
Epoch 0 Test (client-0):        Loss 0.7206 (0.7206)    Prec@1 72.656 (72.656)
Epoch 50        Test (client-0):        Loss 0.6706 (0.7148)    Prec@1 78.906 (76.425)
 * Prec@1 76.540
train_one_ep_time:14.726606845855713 s
feature_infer_one_ep_time:3.0454108715057373 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8879306316375732 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[106/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6505
log--[106/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5937
Epoch 0 Test (client-0):        Loss 0.6715 (0.6715)    Prec@1 78.906 (78.906)
Epoch 50        Test (client-0):        Loss 0.6887 (0.5825)    Prec@1 80.469 (79.979)
 * Prec@1 80.080
train_one_ep_time:12.236318588256836 s
feature_infer_one_ep_time:2.9849660396575928 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9383418560028076 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[107/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5533
log--[107/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5914
Epoch 0 Test (client-0):        Loss 0.6557 (0.6557)    Prec@1 79.688 (79.688)
Epoch 50        Test (client-0):        Loss 0.6631 (0.5687)    Prec@1 82.031 (80.239)
 * Prec@1 80.290
train_one_ep_time:14.148333549499512 s
feature_infer_one_ep_time:2.988511800765991 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9881672859191895 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[108/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5025
log--[108/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5948
Epoch 0 Test (client-0):        Loss 0.5869 (0.5869)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.6126 (0.5838)    Prec@1 84.375 (80.193)
 * Prec@1 80.080
train_one_ep_time:12.181385278701782 s
feature_infer_one_ep_time:3.1411824226379395 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0926845073699951 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[109/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5551
log--[109/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5920
Epoch 0 Test (client-0):        Loss 0.4995 (0.4995)    Prec@1 82.812 (82.812)
Epoch 50        Test (client-0):        Loss 0.6745 (0.5953)    Prec@1 77.344 (79.963)
 * Prec@1 79.950
train_one_ep_time:14.501683235168457 s
feature_infer_one_ep_time:2.991903305053711 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0902817249298096 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[110/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5808
log--[110/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5928
Epoch 0 Test (client-0):        Loss 0.6394 (0.6394)    Prec@1 77.344 (77.344)
Epoch 50        Test (client-0):        Loss 0.6759 (0.6392)    Prec@1 72.656 (78.539)
 * Prec@1 78.910
train_one_ep_time:12.216196060180664 s
feature_infer_one_ep_time:3.22467041015625 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.161109447479248 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[111/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5418
log--[111/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5910
Epoch 0 Test (client-0):        Loss 0.8081 (0.8081)    Prec@1 71.875 (71.875)
Epoch 50        Test (client-0):        Loss 0.6085 (0.6928)    Prec@1 82.031 (76.624)
 * Prec@1 76.480
train_one_ep_time:13.92369818687439 s
feature_infer_one_ep_time:3.035529851913452 s
torch.Size([50048, 8, 8, 8])
the mean of mutal infor is:(-7.951), the est mean of mutal infor is:(-4.089)
feature_clst_one_ep_time:1.1624796390533447 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[112/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4755
log--[112/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5925
Epoch 0 Test (client-0):        Loss 0.7200 (0.7200)    Prec@1 78.125 (78.125)
Epoch 50        Test (client-0):        Loss 0.6811 (0.6288)    Prec@1 79.688 (79.841)
 * Prec@1 79.410
train_one_ep_time:11.8079252243042 s
feature_infer_one_ep_time:3.253063917160034 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9298684597015381 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[113/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5689
log--[113/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5930
Epoch 0 Test (client-0):        Loss 0.6915 (0.6915)    Prec@1 75.781 (75.781)
Epoch 50        Test (client-0):        Loss 0.6148 (0.6269)    Prec@1 82.812 (78.692)
 * Prec@1 78.750
train_one_ep_time:13.803633689880371 s
feature_infer_one_ep_time:3.185525417327881 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9846959114074707 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[114/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4614
log--[114/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5836
Epoch 0 Test (client-0):        Loss 0.6977 (0.6977)    Prec@1 73.438 (73.438)
Epoch 50        Test (client-0):        Loss 0.8235 (0.6290)    Prec@1 78.906 (78.845)
 * Prec@1 78.620
train_one_ep_time:12.109560489654541 s
feature_infer_one_ep_time:3.3975095748901367 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8556926250457764 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[115/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.7187
log--[115/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5852
Epoch 0 Test (client-0):        Loss 0.5737 (0.5737)    Prec@1 79.688 (79.688)
Epoch 50        Test (client-0):        Loss 0.5926 (0.5828)    Prec@1 85.156 (80.040)
 * Prec@1 79.980
train_one_ep_time:13.487440347671509 s
feature_infer_one_ep_time:3.1577038764953613 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0402851104736328 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[116/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5650
log--[116/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5822
Epoch 0 Test (client-0):        Loss 0.6309 (0.6309)    Prec@1 79.688 (79.688)
Epoch 50        Test (client-0):        Loss 0.6591 (0.5835)    Prec@1 83.594 (80.714)
 * Prec@1 80.570
train_one_ep_time:11.835323095321655 s
feature_infer_one_ep_time:3.0462334156036377 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7714216709136963 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[117/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4223
log--[117/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5768
Epoch 0 Test (client-0):        Loss 0.6408 (0.6408)    Prec@1 80.469 (80.469)
Epoch 50        Test (client-0):        Loss 0.6283 (0.6731)    Prec@1 85.938 (77.788)
 * Prec@1 77.880
train_one_ep_time:13.243732929229736 s
feature_infer_one_ep_time:2.958782911300659 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9831638336181641 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[118/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5563
log--[118/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5734
Epoch 0 Test (client-0):        Loss 0.6154 (0.6154)    Prec@1 79.688 (79.688)
Epoch 50        Test (client-0):        Loss 0.7255 (0.6342)    Prec@1 82.031 (79.029)
 * Prec@1 79.200
train_one_ep_time:11.722370386123657 s
feature_infer_one_ep_time:3.419215440750122 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9423811435699463 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[119/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6384
log--[119/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5813
Epoch 0 Test (client-0):        Loss 0.6871 (0.6871)    Prec@1 78.906 (78.906)
Epoch 50        Test (client-0):        Loss 0.7726 (0.6573)    Prec@1 75.781 (77.972)
 * Prec@1 78.140
train_one_ep_time:13.938204526901245 s
feature_infer_one_ep_time:3.1413979530334473 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9126615524291992 s
lambd value is: 0.0 learning rate is: 0.010000000000000002
Train in V2_epoch style
log--[120/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.6535
log--[120/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4862
Epoch 0 Test (client-0):        Loss 0.4572 (0.4572)    Prec@1 87.500 (87.500)
Epoch 50        Test (client-0):        Loss 0.4970 (0.4593)    Prec@1 86.719 (84.528)
 * Prec@1 84.500
best model saved at: 120
train_one_ep_time:11.830341815948486 s
feature_infer_one_ep_time:3.1080446243286133 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9916820526123047 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[121/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3599
log--[121/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4534
Epoch 0 Test (client-0):        Loss 0.5012 (0.5012)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.5329 (0.4647)    Prec@1 85.938 (84.452)
 * Prec@1 84.680
best model saved at: 121
train_one_ep_time:14.671380043029785 s
feature_infer_one_ep_time:3.2173843383789062 s
torch.Size([50048, 8, 8, 8])
the mean of mutal infor is:(-8.151), the est mean of mutal infor is:(-4.255)
feature_clst_one_ep_time:0.9129030704498291 s
/home/asher/anaconda3/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.
  warnings.warn(
./saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125/pretrain_False_lambd_0_noise_0.01_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125//visualize/121.png
lambd value is: 0.0 learning rate is: 0.0020000000000000005
/home/asher/anaconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Train in V2_epoch style
/home/asher/CEM/model_training_paral_pruning.py:1720: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_loss_list.append(torch.tensor(train_loss))
log--[122/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2990
log--[122/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4482
Epoch 0 Test (client-0):        Loss 0.4544 (0.4544)    Prec@1 87.500 (87.500)
Epoch 50        Test (client-0):        Loss 0.5008 (0.4611)    Prec@1 86.719 (84.942)
 * Prec@1 84.960
best model saved at: 122
train_one_ep_time:14.347609281539917 s
feature_infer_one_ep_time:3.038604497909546 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8920140266418457 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[123/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3863
log--[123/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4394
Epoch 0 Test (client-0):        Loss 0.4922 (0.4922)    Prec@1 84.375 (84.375)
Epoch 50        Test (client-0):        Loss 0.5005 (0.4718)    Prec@1 85.938 (84.605)
 * Prec@1 84.590
train_one_ep_time:12.180154800415039 s
feature_infer_one_ep_time:3.045711040496826 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7045574188232422 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[124/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4462
log--[124/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4343
Epoch 0 Test (client-0):        Loss 0.4676 (0.4676)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.5367 (0.4619)    Prec@1 85.156 (84.651)
 * Prec@1 84.540
train_one_ep_time:14.788732767105103 s
feature_infer_one_ep_time:3.190012216567993 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8366973400115967 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[125/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3985
log--[125/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4285
Epoch 0 Test (client-0):        Loss 0.4678 (0.4678)    Prec@1 86.719 (86.719)
Epoch 50        Test (client-0):        Loss 0.5345 (0.4657)    Prec@1 86.719 (84.482)
 * Prec@1 84.540
train_one_ep_time:12.150433540344238 s
feature_infer_one_ep_time:3.149852991104126 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7854764461517334 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[126/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3572
log--[126/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4190
Epoch 0 Test (client-0):        Loss 0.4748 (0.4748)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.5624 (0.4719)    Prec@1 85.156 (84.206)
 * Prec@1 84.380
train_one_ep_time:12.547926187515259 s
feature_infer_one_ep_time:3.3358054161071777 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6295790672302246 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[127/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3827
log--[127/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4154
Epoch 0 Test (client-0):        Loss 0.4707 (0.4707)    Prec@1 87.500 (87.500)
Epoch 50        Test (client-0):        Loss 0.5746 (0.4790)    Prec@1 82.031 (84.206)
 * Prec@1 84.270
train_one_ep_time:14.71568512916565 s
feature_infer_one_ep_time:3.0906779766082764 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.018721580505371 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[128/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3134
log--[128/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4131
Epoch 0 Test (client-0):        Loss 0.5263 (0.5263)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.5152 (0.4840)    Prec@1 85.156 (84.069)
 * Prec@1 83.990
train_one_ep_time:12.197007417678833 s
feature_infer_one_ep_time:3.14501690864563 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8516452312469482 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[129/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4215
log--[129/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4094
Epoch 0 Test (client-0):        Loss 0.4584 (0.4584)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.4952 (0.4688)    Prec@1 85.156 (84.620)
 * Prec@1 84.730
train_one_ep_time:13.296884536743164 s
feature_infer_one_ep_time:2.874302625656128 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7688758373260498 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[130/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3682
log--[130/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4076
Epoch 0 Test (client-0):        Loss 0.5091 (0.5091)    Prec@1 87.500 (87.500)
Epoch 50        Test (client-0):        Loss 0.5844 (0.4852)    Prec@1 83.594 (84.206)
 * Prec@1 84.220
train_one_ep_time:11.943411111831665 s
feature_infer_one_ep_time:3.1837220191955566 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7158277034759521 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[131/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4294
log--[131/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4050
Epoch 0 Test (client-0):        Loss 0.5004 (0.5004)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.5469 (0.4748)    Prec@1 84.375 (84.528)
 * Prec@1 84.590
train_one_ep_time:15.039069890975952 s
feature_infer_one_ep_time:2.9312474727630615 s
torch.Size([50048, 8, 8, 8])
/home/asher/CEM/model_training_paral_pruning.py:1909: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  rob_list.append(torch.tensor(log_det_mean.detach().cpu()))
the mean of mutal infor is:(-8.326), the est mean of mutal infor is:(-4.479)
feature_clst_one_ep_time:0.8205413818359375 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[132/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5664
log--[132/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4061
Epoch 0 Test (client-0):        Loss 0.5423 (0.5423)    Prec@1 87.500 (87.500)
Epoch 50        Test (client-0):        Loss 0.5000 (0.4980)    Prec@1 87.500 (83.701)
 * Prec@1 83.950
train_one_ep_time:12.156405210494995 s
feature_infer_one_ep_time:3.320756196975708 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7307620048522949 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[133/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3634
log--[133/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4022
Epoch 0 Test (client-0):        Loss 0.4938 (0.4938)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.4970 (0.4706)    Prec@1 86.719 (85.080)
 * Prec@1 85.090
best model saved at: 133
train_one_ep_time:15.174683809280396 s
feature_infer_one_ep_time:2.9363014698028564 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7487266063690186 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[134/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3019
log--[134/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3997
Epoch 0 Test (client-0):        Loss 0.5023 (0.5023)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.5818 (0.4924)    Prec@1 82.812 (84.038)
 * Prec@1 84.300
train_one_ep_time:11.603638887405396 s
feature_infer_one_ep_time:3.1042566299438477 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7729814052581787 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[135/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3763
log--[135/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4026
Epoch 0 Test (client-0):        Loss 0.4472 (0.4472)    Prec@1 86.719 (86.719)
Epoch 50        Test (client-0):        Loss 0.4770 (0.4777)    Prec@1 85.156 (84.681)
 * Prec@1 84.680
train_one_ep_time:14.964089155197144 s
feature_infer_one_ep_time:2.947305679321289 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8642220497131348 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[136/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3887
log--[136/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3940
Epoch 0 Test (client-0):        Loss 0.4888 (0.4888)    Prec@1 86.719 (86.719)
Epoch 50        Test (client-0):        Loss 0.4844 (0.4770)    Prec@1 87.500 (84.528)
 * Prec@1 84.910
train_one_ep_time:11.871595621109009 s
feature_infer_one_ep_time:3.1098623275756836 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7081940174102783 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[137/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4502
log--[137/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3894
Epoch 0 Test (client-0):        Loss 0.4884 (0.4884)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.5473 (0.4993)    Prec@1 82.031 (83.992)
 * Prec@1 84.140
train_one_ep_time:14.411251306533813 s
feature_infer_one_ep_time:3.0075430870056152 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7937564849853516 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[138/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3422
log--[138/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3878
Epoch 0 Test (client-0):        Loss 0.4807 (0.4807)    Prec@1 88.281 (88.281)
Epoch 50        Test (client-0):        Loss 0.5006 (0.4841)    Prec@1 84.375 (84.605)
 * Prec@1 84.700
train_one_ep_time:11.718667268753052 s
feature_infer_one_ep_time:3.504784107208252 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.959402322769165 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[139/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3901
log--[139/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3894
Epoch 0 Test (client-0):        Loss 0.4770 (0.4770)    Prec@1 87.500 (87.500)
Epoch 50        Test (client-0):        Loss 0.4964 (0.4742)    Prec@1 86.719 (85.018)
 * Prec@1 84.980
train_one_ep_time:14.483269691467285 s
feature_infer_one_ep_time:2.818779468536377 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.973461389541626 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[140/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2807
log--[140/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3874
Epoch 0 Test (client-0):        Loss 0.4939 (0.4939)    Prec@1 86.719 (86.719)
Epoch 50        Test (client-0):        Loss 0.4542 (0.5026)    Prec@1 89.062 (83.900)
 * Prec@1 84.060
train_one_ep_time:11.174245119094849 s
feature_infer_one_ep_time:2.8626811504364014 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8369975090026855 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[141/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4093
log--[141/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3854
Epoch 0 Test (client-0):        Loss 0.5256 (0.5256)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.4731 (0.4952)    Prec@1 88.281 (84.589)
 * Prec@1 84.610
train_one_ep_time:12.398082256317139 s
feature_infer_one_ep_time:3.31742787361145 s
torch.Size([50048, 8, 8, 8])
the mean of mutal infor is:(-8.376), the est mean of mutal infor is:(-4.576)
feature_clst_one_ep_time:1.0604667663574219 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[142/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2892
log--[142/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3868
Epoch 0 Test (client-0):        Loss 0.5038 (0.5038)    Prec@1 87.500 (87.500)
Epoch 50        Test (client-0):        Loss 0.5049 (0.4972)    Prec@1 86.719 (84.099)
 * Prec@1 84.290
train_one_ep_time:15.879743099212646 s
feature_infer_one_ep_time:3.128843307495117 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.690838098526001 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[143/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4633
log--[143/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3798
Epoch 0 Test (client-0):        Loss 0.5148 (0.5148)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.4258 (0.4802)    Prec@1 90.625 (84.482)
 * Prec@1 84.470
train_one_ep_time:11.980485200881958 s
feature_infer_one_ep_time:2.97737193107605 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6072437763214111 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[144/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3211
log--[144/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3822
Epoch 0 Test (client-0):        Loss 0.4925 (0.4925)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.5125 (0.4906)    Prec@1 84.375 (84.482)
 * Prec@1 84.580
train_one_ep_time:13.915861129760742 s
feature_infer_one_ep_time:3.1070401668548584 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8149757385253906 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[145/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3832
log--[145/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3761
Epoch 0 Test (client-0):        Loss 0.5220 (0.5220)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.5241 (0.4818)    Prec@1 82.031 (84.498)
 * Prec@1 84.550
train_one_ep_time:12.019119501113892 s
feature_infer_one_ep_time:3.2173190116882324 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8712165355682373 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[146/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3188
log--[146/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3782
Epoch 0 Test (client-0):        Loss 0.5500 (0.5500)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.5008 (0.5136)    Prec@1 86.719 (83.762)
 * Prec@1 84.060
train_one_ep_time:15.367394924163818 s
feature_infer_one_ep_time:3.0841856002807617 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6094682216644287 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[147/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4770
log--[147/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3767
Epoch 0 Test (client-0):        Loss 0.5366 (0.5366)    Prec@1 84.375 (84.375)
Epoch 50        Test (client-0):        Loss 0.4651 (0.4794)    Prec@1 87.500 (84.681)
 * Prec@1 84.700
train_one_ep_time:11.991583585739136 s
feature_infer_one_ep_time:3.1838223934173584 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:2.563035011291504 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[148/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2538
log--[148/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3659
Epoch 0 Test (client-0):        Loss 0.5167 (0.5167)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.5345 (0.5270)    Prec@1 84.375 (84.145)
 * Prec@1 84.400
train_one_ep_time:12.179004907608032 s
feature_infer_one_ep_time:3.007906198501587 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7807748317718506 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[149/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4049
log--[149/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3709
Epoch 0 Test (client-0):        Loss 0.4863 (0.4863)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.4923 (0.4994)    Prec@1 85.156 (84.145)
 * Prec@1 84.340
train_one_ep_time:11.994961261749268 s
feature_infer_one_ep_time:3.2454025745391846 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:2.774204730987549 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[150/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3668
log--[150/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3711
Epoch 0 Test (client-0):        Loss 0.5506 (0.5506)    Prec@1 82.812 (82.812)
Epoch 50        Test (client-0):        Loss 0.5357 (0.5171)    Prec@1 84.375 (83.900)
 * Prec@1 84.130
train_one_ep_time:12.316811323165894 s
feature_infer_one_ep_time:3.0397255420684814 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7811064720153809 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[151/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.5438
log--[151/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3723
Epoch 0 Test (client-0):        Loss 0.5468 (0.5468)    Prec@1 84.375 (84.375)
Epoch 50        Test (client-0):        Loss 0.4819 (0.5391)    Prec@1 83.594 (83.104)
 * Prec@1 83.430
train_one_ep_time:11.894812822341919 s
feature_infer_one_ep_time:3.2355144023895264 s
torch.Size([50048, 8, 8, 8])
the mean of mutal infor is:(-8.413), the est mean of mutal infor is:(-4.657)
feature_clst_one_ep_time:3.019981622695923 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[152/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3091
log--[152/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3644
Epoch 0 Test (client-0):        Loss 0.5239 (0.5239)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.5097 (0.5151)    Prec@1 85.156 (83.701)
 * Prec@1 84.100
train_one_ep_time:12.284021854400635 s
feature_infer_one_ep_time:3.124675989151001 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7925534248352051 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[153/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3034
log--[153/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3653
Epoch 0 Test (client-0):        Loss 0.4831 (0.4831)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.5776 (0.4971)    Prec@1 81.250 (84.421)
 * Prec@1 84.350
train_one_ep_time:12.12500262260437 s
feature_infer_one_ep_time:5.052647113800049 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.001760482788086 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[154/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3326
log--[154/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3596
Epoch 0 Test (client-0):        Loss 0.5371 (0.5371)    Prec@1 82.812 (82.812)
Epoch 50        Test (client-0):        Loss 0.5182 (0.5130)    Prec@1 86.719 (84.007)
 * Prec@1 83.990
train_one_ep_time:12.292307376861572 s
feature_infer_one_ep_time:3.122969150543213 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7962000370025635 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[155/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2534
log--[155/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3657
Epoch 0 Test (client-0):        Loss 0.5266 (0.5266)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.5365 (0.4925)    Prec@1 83.594 (84.161)
 * Prec@1 84.200
train_one_ep_time:12.210137844085693 s
feature_infer_one_ep_time:5.063879013061523 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7981526851654053 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[156/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4356
log--[156/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3594
Epoch 0 Test (client-0):        Loss 0.5139 (0.5139)    Prec@1 82.812 (82.812)
Epoch 50        Test (client-0):        Loss 0.5337 (0.5006)    Prec@1 84.375 (83.762)
 * Prec@1 84.100
train_one_ep_time:12.250189781188965 s
feature_infer_one_ep_time:2.916942834854126 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7872090339660645 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[157/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3650
log--[157/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3585
Epoch 0 Test (client-0):        Loss 0.5326 (0.5326)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.5528 (0.5212)    Prec@1 85.938 (83.670)
 * Prec@1 83.760
train_one_ep_time:12.300916910171509 s
feature_infer_one_ep_time:4.69771933555603 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8439891338348389 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[158/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3311
log--[158/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3498
Epoch 0 Test (client-0):        Loss 0.5729 (0.5729)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.5907 (0.5474)    Prec@1 82.031 (83.333)
 * Prec@1 83.480
train_one_ep_time:12.389122724533081 s
feature_infer_one_ep_time:3.0129449367523193 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.690018892288208 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[159/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3512
log--[159/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3608
Epoch 0 Test (client-0):        Loss 0.4830 (0.4830)    Prec@1 86.719 (86.719)
Epoch 50        Test (client-0):        Loss 0.5529 (0.4915)    Prec@1 84.375 (84.467)
 * Prec@1 84.550
train_one_ep_time:12.525435209274292 s
feature_infer_one_ep_time:3.044236660003662 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8532280921936035 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[160/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3119
log--[160/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3524
Epoch 0 Test (client-0):        Loss 0.5870 (0.5870)    Prec@1 82.031 (82.031)
Epoch 50        Test (client-0):        Loss 0.5698 (0.5237)    Prec@1 84.375 (83.824)
 * Prec@1 83.700
train_one_ep_time:12.369799375534058 s
feature_infer_one_ep_time:5.927618503570557 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.088820219039917 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[161/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2709
log--[161/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3544
Epoch 0 Test (client-0):        Loss 0.5352 (0.5352)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.5027 (0.5283)    Prec@1 82.031 (84.099)
 * Prec@1 84.050
train_one_ep_time:11.324116230010986 s
feature_infer_one_ep_time:2.939577341079712 s
torch.Size([50048, 8, 8, 8])
the mean of mutal infor is:(-8.468), the est mean of mutal infor is:(-4.767)
feature_clst_one_ep_time:0.8562700748443604 s
/home/asher/anaconda3/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.
  warnings.warn(
./saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125/pretrain_False_lambd_0_noise_0.01_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125//visualize/161.png
lambd value is: 0.0 learning rate is: 0.0020000000000000005
/home/asher/anaconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Train in V2_epoch style
/home/asher/CEM/model_training_paral_pruning.py:1720: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_loss_list.append(torch.tensor(train_loss))
log--[162/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3405
log--[162/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3449
Epoch 0 Test (client-0):        Loss 0.5457 (0.5457)    Prec@1 81.250 (81.250)
Epoch 50        Test (client-0):        Loss 0.5049 (0.5146)    Prec@1 82.812 (84.191)
 * Prec@1 84.070
train_one_ep_time:11.364619970321655 s
feature_infer_one_ep_time:2.8356990814208984 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8894612789154053 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[163/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2944
log--[163/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3490
Epoch 0 Test (client-0):        Loss 0.5550 (0.5550)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.4362 (0.5040)    Prec@1 84.375 (84.651)
 * Prec@1 84.660
train_one_ep_time:11.997917652130127 s
feature_infer_one_ep_time:3.1159727573394775 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.846994161605835 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[164/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3014
log--[164/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3401
Epoch 0 Test (client-0):        Loss 0.5293 (0.5293)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.4995 (0.5516)    Prec@1 85.938 (83.609)
 * Prec@1 83.580
train_one_ep_time:12.196484804153442 s
feature_infer_one_ep_time:2.9483110904693604 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8279635906219482 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[165/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2123
log--[165/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3472
Epoch 0 Test (client-0):        Loss 0.6529 (0.6529)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.5689 (0.5533)    Prec@1 82.031 (83.808)
 * Prec@1 83.920
train_one_ep_time:12.090662240982056 s
feature_infer_one_ep_time:3.1467275619506836 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8162858486175537 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[166/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3028
log--[166/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3468
Epoch 0 Test (client-0):        Loss 0.5637 (0.5637)    Prec@1 82.812 (82.812)
Epoch 50        Test (client-0):        Loss 0.6159 (0.5279)    Prec@1 79.688 (83.716)
 * Prec@1 84.110
train_one_ep_time:11.716235160827637 s
feature_infer_one_ep_time:3.011979579925537 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7860689163208008 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[167/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4436
log--[167/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3391
Epoch 0 Test (client-0):        Loss 0.4897 (0.4897)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.5846 (0.5287)    Prec@1 85.156 (83.931)
 * Prec@1 83.920
train_one_ep_time:12.43517780303955 s
feature_infer_one_ep_time:3.013077974319458 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8256485462188721 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[168/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2792
log--[168/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3400
Epoch 0 Test (client-0):        Loss 0.5519 (0.5519)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.5970 (0.5257)    Prec@1 81.250 (83.563)
 * Prec@1 83.820
train_one_ep_time:13.307952404022217 s
feature_infer_one_ep_time:2.9769234657287598 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8279399871826172 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[169/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2598
log--[169/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3400
Epoch 0 Test (client-0):        Loss 0.5067 (0.5067)    Prec@1 80.469 (80.469)
Epoch 50        Test (client-0):        Loss 0.6056 (0.5497)    Prec@1 82.812 (83.349)
 * Prec@1 83.490
train_one_ep_time:11.467718601226807 s
feature_infer_one_ep_time:3.097334146499634 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.674119234085083 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[170/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3170
log--[170/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3375
Epoch 0 Test (client-0):        Loss 0.5213 (0.5213)    Prec@1 82.031 (82.031)
Epoch 50        Test (client-0):        Loss 0.5259 (0.5388)    Prec@1 84.375 (83.839)
 * Prec@1 83.930
train_one_ep_time:13.16087818145752 s
feature_infer_one_ep_time:2.9838292598724365 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7429373264312744 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[171/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2162
log--[171/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3332
Epoch 0 Test (client-0):        Loss 0.5685 (0.5685)    Prec@1 86.719 (86.719)
Epoch 50        Test (client-0):        Loss 0.6078 (0.5645)    Prec@1 82.031 (83.670)
 * Prec@1 83.500
train_one_ep_time:11.401682615280151 s
feature_infer_one_ep_time:2.954298973083496 s
torch.Size([50048, 8, 8, 8])
/home/asher/CEM/model_training_paral_pruning.py:1909: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  rob_list.append(torch.tensor(log_det_mean.detach().cpu()))
the mean of mutal infor is:(-8.458), the est mean of mutal infor is:(-4.763)
feature_clst_one_ep_time:1.0753850936889648 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[172/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3799
log--[172/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3339
Epoch 0 Test (client-0):        Loss 0.5630 (0.5630)    Prec@1 82.031 (82.031)
Epoch 50        Test (client-0):        Loss 0.5904 (0.5537)    Prec@1 82.031 (83.088)
 * Prec@1 83.050
train_one_ep_time:13.08724308013916 s
feature_infer_one_ep_time:2.908935070037842 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7992312908172607 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[173/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3140
log--[173/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3342
Epoch 0 Test (client-0):        Loss 0.5907 (0.5907)    Prec@1 82.031 (82.031)
Epoch 50        Test (client-0):        Loss 0.5108 (0.5362)    Prec@1 82.031 (83.732)
 * Prec@1 84.000
train_one_ep_time:11.236090898513794 s
feature_infer_one_ep_time:2.9250411987304688 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7773027420043945 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[174/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3174
log--[174/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3323
Epoch 0 Test (client-0):        Loss 0.6219 (0.6219)    Prec@1 82.812 (82.812)
Epoch 50        Test (client-0):        Loss 0.6255 (0.5818)    Prec@1 80.469 (82.598)
 * Prec@1 82.770
train_one_ep_time:13.054437637329102 s
feature_infer_one_ep_time:2.9877476692199707 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7462964057922363 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[175/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3119
log--[175/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3268
Epoch 0 Test (client-0):        Loss 0.6787 (0.6787)    Prec@1 82.031 (82.031)
Epoch 50        Test (client-0):        Loss 0.6352 (0.5875)    Prec@1 82.031 (82.874)
 * Prec@1 82.710
train_one_ep_time:11.333704710006714 s
feature_infer_one_ep_time:2.9038333892822266 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.839907169342041 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[176/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2564
log--[176/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3297
Epoch 0 Test (client-0):        Loss 0.5453 (0.5453)    Prec@1 86.719 (86.719)
Epoch 50        Test (client-0):        Loss 0.5165 (0.5286)    Prec@1 84.375 (84.206)
 * Prec@1 84.320
train_one_ep_time:13.143606185913086 s
feature_infer_one_ep_time:2.982896089553833 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7644317150115967 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[177/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3714
log--[177/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3210
Epoch 0 Test (client-0):        Loss 0.4863 (0.4863)    Prec@1 86.719 (86.719)
Epoch 50        Test (client-0):        Loss 0.4609 (0.5198)    Prec@1 89.844 (84.344)
 * Prec@1 84.490
train_one_ep_time:11.582499504089355 s
feature_infer_one_ep_time:2.861494541168213 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6566159725189209 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[178/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2127
log--[178/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3263
Epoch 0 Test (client-0):        Loss 0.4737 (0.4737)    Prec@1 87.500 (87.500)
Epoch 50        Test (client-0):        Loss 0.5871 (0.5334)    Prec@1 83.594 (84.038)
 * Prec@1 83.850
train_one_ep_time:12.812721252441406 s
feature_infer_one_ep_time:2.8420910835266113 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8374607563018799 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[179/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2265
log--[179/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.3208
Epoch 0 Test (client-0):        Loss 0.5508 (0.5508)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.6038 (0.5391)    Prec@1 83.594 (83.762)
 * Prec@1 83.670
train_one_ep_time:11.21646785736084 s
feature_infer_one_ep_time:2.9044384956359863 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7693924903869629 s
lambd value is: 0.0 learning rate is: 0.0020000000000000005
Train in V2_epoch style
log--[180/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.4402
log--[180/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2673
Epoch 0 Test (client-0):        Loss 0.4855 (0.4855)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.5004 (0.4991)    Prec@1 83.594 (85.340)
 * Prec@1 85.320
best model saved at: 180
train_one_ep_time:13.330378532409668 s
feature_infer_one_ep_time:2.890125274658203 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7896621227264404 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[181/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2124
log--[181/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2425
Epoch 0 Test (client-0):        Loss 0.5573 (0.5573)    Prec@1 86.719 (86.719)
Epoch 50        Test (client-0):        Loss 0.5164 (0.5209)    Prec@1 86.719 (85.126)
 * Prec@1 85.090
train_one_ep_time:11.329658508300781 s
feature_infer_one_ep_time:2.990222454071045 s
torch.Size([50048, 8, 8, 8])
the mean of mutal infor is:(-8.476), the est mean of mutal infor is:(-4.819)
feature_clst_one_ep_time:0.8512406349182129 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[182/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2740
log--[182/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2365
Epoch 0 Test (client-0):        Loss 0.5455 (0.5455)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.5344 (0.5239)    Prec@1 85.938 (85.248)
 * Prec@1 85.390
best model saved at: 182
train_one_ep_time:11.418212413787842 s
feature_infer_one_ep_time:4.621363401412964 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6612105369567871 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[183/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1807
log--[183/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2234
Epoch 0 Test (client-0):        Loss 0.5228 (0.5228)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.5735 (0.5357)    Prec@1 85.938 (85.126)
 * Prec@1 85.160
train_one_ep_time:11.460105895996094 s
feature_infer_one_ep_time:2.850109338760376 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6468284130096436 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[184/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2479
log--[184/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2257
Epoch 0 Test (client-0):        Loss 0.5558 (0.5558)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.5417 (0.5404)    Prec@1 86.719 (85.156)
 * Prec@1 85.100
train_one_ep_time:11.415371894836426 s
feature_infer_one_ep_time:2.885194778442383 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:2.344879388809204 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[185/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2354
log--[185/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2220
Epoch 0 Test (client-0):        Loss 0.5729 (0.5729)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.5731 (0.5599)    Prec@1 84.375 (84.850)
 * Prec@1 84.750
train_one_ep_time:11.388236284255981 s
feature_infer_one_ep_time:2.8950374126434326 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.4900333881378174 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[186/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1582
log--[186/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2183
Epoch 0 Test (client-0):        Loss 0.5678 (0.5678)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.5246 (0.5591)    Prec@1 83.594 (84.881)
 * Prec@1 84.900
train_one_ep_time:11.418211460113525 s
feature_infer_one_ep_time:2.9451427459716797 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7234373092651367 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[187/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2311
log--[187/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2161
Epoch 0 Test (client-0):        Loss 0.5885 (0.5885)    Prec@1 84.375 (84.375)
Epoch 50        Test (client-0):        Loss 0.6009 (0.5567)    Prec@1 82.812 (84.666)
 * Prec@1 84.650
train_one_ep_time:13.069581508636475 s
feature_infer_one_ep_time:2.8569889068603516 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6322231292724609 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[188/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1691
log--[188/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2115
Epoch 0 Test (client-0):        Loss 0.5409 (0.5409)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.6222 (0.5665)    Prec@1 83.594 (84.957)
 * Prec@1 84.900
train_one_ep_time:11.512101888656616 s
feature_infer_one_ep_time:2.9798998832702637 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6183772087097168 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[189/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1973
log--[189/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2133
Epoch 0 Test (client-0):        Loss 0.6147 (0.6147)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.5878 (0.5654)    Prec@1 83.594 (84.620)
 * Prec@1 84.610
train_one_ep_time:13.09309720993042 s
feature_infer_one_ep_time:2.9012014865875244 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.579498291015625 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[190/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1263
log--[190/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2095
Epoch 0 Test (client-0):        Loss 0.6473 (0.6473)    Prec@1 86.719 (86.719)
Epoch 50        Test (client-0):        Loss 0.6206 (0.5687)    Prec@1 83.594 (84.467)
 * Prec@1 84.660
train_one_ep_time:11.336872100830078 s
feature_infer_one_ep_time:2.9969961643218994 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8082497119903564 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[191/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2068
log--[191/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2042
Epoch 0 Test (client-0):        Loss 0.6675 (0.6675)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.5740 (0.5699)    Prec@1 84.375 (84.865)
 * Prec@1 84.820
train_one_ep_time:13.130275964736938 s
feature_infer_one_ep_time:2.9252142906188965 s
torch.Size([50048, 8, 8, 8])
the mean of mutal infor is:(-8.477), the est mean of mutal infor is:(-4.832)
feature_clst_one_ep_time:0.7715518474578857 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[192/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1170
log--[192/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2030
Epoch 0 Test (client-0):        Loss 0.6515 (0.6515)    Prec@1 84.375 (84.375)
Epoch 50        Test (client-0):        Loss 0.6507 (0.5771)    Prec@1 83.594 (84.620)
 * Prec@1 84.720
train_one_ep_time:11.424788475036621 s
feature_infer_one_ep_time:2.8888306617736816 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.5513594150543213 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[193/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1934
log--[193/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1969
Epoch 0 Test (client-0):        Loss 0.6231 (0.6231)    Prec@1 84.375 (84.375)
Epoch 50        Test (client-0):        Loss 0.5988 (0.5903)    Prec@1 85.938 (84.436)
 * Prec@1 84.620
train_one_ep_time:11.525704145431519 s
feature_infer_one_ep_time:2.8956830501556396 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7617344856262207 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[194/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2031
log--[194/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1999
Epoch 0 Test (client-0):        Loss 0.6771 (0.6771)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.5948 (0.5876)    Prec@1 83.594 (84.620)
 * Prec@1 84.590
train_one_ep_time:14.094168186187744 s
feature_infer_one_ep_time:2.7882497310638428 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8617913722991943 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[195/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1036
log--[195/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1976
Epoch 0 Test (client-0):        Loss 0.6722 (0.6722)    Prec@1 82.031 (82.031)
Epoch 50        Test (client-0):        Loss 0.5825 (0.5877)    Prec@1 82.812 (84.559)
 * Prec@1 84.750
train_one_ep_time:11.460755825042725 s
feature_infer_one_ep_time:2.928394317626953 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.5852463245391846 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[196/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2359
log--[196/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1936
Epoch 0 Test (client-0):        Loss 0.6564 (0.6564)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.6343 (0.6028)    Prec@1 82.812 (84.528)
 * Prec@1 84.540
train_one_ep_time:13.697286367416382 s
feature_infer_one_ep_time:2.8585147857666016 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7413856983184814 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[197/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2501
log--[197/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1891
Epoch 0 Test (client-0):        Loss 0.6766 (0.6766)    Prec@1 82.812 (82.812)
Epoch 50        Test (client-0):        Loss 0.6533 (0.5913)    Prec@1 82.031 (84.697)
 * Prec@1 84.830
train_one_ep_time:11.42380952835083 s
feature_infer_one_ep_time:2.9095702171325684 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6732356548309326 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[198/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2285
log--[198/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1914
Epoch 0 Test (client-0):        Loss 0.6541 (0.6541)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.6335 (0.5901)    Prec@1 83.594 (84.942)
 * Prec@1 84.800
train_one_ep_time:13.17857813835144 s
feature_infer_one_ep_time:2.959393262863159 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6407229900360107 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[199/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2128
log--[199/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1897
Epoch 0 Test (client-0):        Loss 0.6083 (0.6083)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.6503 (0.5856)    Prec@1 81.250 (85.034)
 * Prec@1 84.900
train_one_ep_time:11.35806655883789 s
feature_infer_one_ep_time:2.881481409072876 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7307672500610352 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[200/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1913
log--[200/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1919
Epoch 0 Test (client-0):        Loss 0.6194 (0.6194)    Prec@1 84.375 (84.375)
Epoch 50        Test (client-0):        Loss 0.6252 (0.5910)    Prec@1 82.031 (84.528)
 * Prec@1 84.550
train_one_ep_time:13.085257530212402 s
feature_infer_one_ep_time:2.971142053604126 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6299083232879639 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[201/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2490
log--[201/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1842
Epoch 0 Test (client-0):        Loss 0.6161 (0.6161)    Prec@1 87.500 (87.500)
Epoch 50        Test (client-0):        Loss 0.5911 (0.5929)    Prec@1 85.938 (84.881)
 * Prec@1 84.860
best model saved at: 201
train_one_ep_time:11.308119773864746 s
feature_infer_one_ep_time:2.9514081478118896 s
torch.Size([50048, 8, 8, 8])
the mean of mutal infor is:(-8.476), the est mean of mutal infor is:(-4.830)
feature_clst_one_ep_time:0.9066317081451416 s
/home/asher/anaconda3/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.
  warnings.warn(
./saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125/pretrain_False_lambd_0_noise_0.01_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125//visualize/201.png
lambd value is: 0.0 learning rate is: 0.00040000000000000013
/home/asher/anaconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Train in V2_epoch style
/home/asher/CEM/model_training_paral_pruning.py:1720: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_loss_list.append(torch.tensor(train_loss))
log--[202/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2123
log--[202/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1844
Epoch 0 Test (client-0):        Loss 0.7144 (0.7144)    Prec@1 84.375 (84.375)
Epoch 50        Test (client-0):        Loss 0.5506 (0.6117)    Prec@1 85.156 (84.467)
 * Prec@1 84.700
train_one_ep_time:11.551315307617188 s
feature_infer_one_ep_time:3.0103070735931396 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8114879131317139 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[203/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2638
log--[203/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1837
Epoch 0 Test (client-0):        Loss 0.6597 (0.6597)    Prec@1 82.812 (82.812)
Epoch 50        Test (client-0):        Loss 0.5850 (0.6124)    Prec@1 85.156 (84.528)
 * Prec@1 84.570
train_one_ep_time:11.587583303451538 s
feature_infer_one_ep_time:2.9754767417907715 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7770674228668213 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[204/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1572
log--[204/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1820
Epoch 0 Test (client-0):        Loss 0.6231 (0.6231)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.6444 (0.6111)    Prec@1 84.375 (84.452)
 * Prec@1 84.300
train_one_ep_time:13.443115711212158 s
feature_infer_one_ep_time:2.956569194793701 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.731433629989624 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[205/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1587
log--[205/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1805
Epoch 0 Test (client-0):        Loss 0.6472 (0.6472)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.6450 (0.6182)    Prec@1 82.812 (84.528)
 * Prec@1 84.500
train_one_ep_time:11.581831932067871 s
feature_infer_one_ep_time:2.7838964462280273 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.5737154483795166 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[206/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1496
log--[206/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1764
Epoch 0 Test (client-0):        Loss 0.7720 (0.7720)    Prec@1 82.031 (82.031)
Epoch 50        Test (client-0):        Loss 0.6582 (0.6341)    Prec@1 82.812 (84.727)
 * Prec@1 84.530
train_one_ep_time:13.315958499908447 s
feature_infer_one_ep_time:3.0119035243988037 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.520094633102417 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[207/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.0876
log--[207/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1768
Epoch 0 Test (client-0):        Loss 0.6860 (0.6860)    Prec@1 84.375 (84.375)
Epoch 50        Test (client-0):        Loss 0.6985 (0.6206)    Prec@1 82.031 (84.344)
 * Prec@1 84.470
train_one_ep_time:11.575741291046143 s
feature_infer_one_ep_time:2.924503803253174 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7100889682769775 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[208/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1352
log--[208/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1799
Epoch 0 Test (client-0):        Loss 0.7113 (0.7113)    Prec@1 82.812 (82.812)
Epoch 50        Test (client-0):        Loss 0.6205 (0.6130)    Prec@1 85.938 (84.528)
 * Prec@1 84.610
train_one_ep_time:13.31446623802185 s
feature_infer_one_ep_time:3.0108842849731445 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8035850524902344 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[209/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1700
log--[209/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1687
Epoch 0 Test (client-0):        Loss 0.6913 (0.6913)    Prec@1 84.375 (84.375)
Epoch 50        Test (client-0):        Loss 0.6650 (0.6253)    Prec@1 82.031 (84.390)
 * Prec@1 84.310
train_one_ep_time:11.389767169952393 s
feature_infer_one_ep_time:3.1128365993499756 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6305856704711914 s
lambd value is: 0.0 learning rate is: 0.00040000000000000013
Train in V2_epoch style
log--[210/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1404
log--[210/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1599
Epoch 0 Test (client-0):        Loss 0.7096 (0.7096)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.6321 (0.6130)    Prec@1 83.594 (84.727)
 * Prec@1 84.680
train_one_ep_time:13.174782752990723 s
feature_infer_one_ep_time:3.0182363986968994 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7173001766204834 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[211/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.0651
log--[211/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1567
Epoch 0 Test (client-0):        Loss 0.6860 (0.6860)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.6422 (0.6120)    Prec@1 83.594 (85.003)
 * Prec@1 84.840
train_one_ep_time:11.414829730987549 s
feature_infer_one_ep_time:3.067652940750122 s
torch.Size([50048, 8, 8, 8])
/home/asher/CEM/model_training_paral_pruning.py:1909: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  rob_list.append(torch.tensor(log_det_mean.detach().cpu()))
the mean of mutal infor is:(-8.479), the est mean of mutal infor is:(-4.843)
feature_clst_one_ep_time:0.8865633010864258 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[212/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1620
log--[212/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1517
Epoch 0 Test (client-0):        Loss 0.6920 (0.6920)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.6359 (0.6206)    Prec@1 85.156 (84.911)
 * Prec@1 84.920
best model saved at: 212
train_one_ep_time:13.282034397125244 s
feature_infer_one_ep_time:3.029633045196533 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9265248775482178 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[213/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.2016
log--[213/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1530
Epoch 0 Test (client-0):        Loss 0.6791 (0.6791)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.6236 (0.6158)    Prec@1 84.375 (84.789)
 * Prec@1 84.820
train_one_ep_time:11.492212772369385 s
feature_infer_one_ep_time:2.9006195068359375 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8925726413726807 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[214/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1612
log--[214/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1462
Epoch 0 Test (client-0):        Loss 0.7468 (0.7468)    Prec@1 84.375 (84.375)
Epoch 50        Test (client-0):        Loss 0.6512 (0.6219)    Prec@1 83.594 (85.141)
 * Prec@1 85.130
best model saved at: 214
train_one_ep_time:13.240055322647095 s
feature_infer_one_ep_time:2.9256715774536133 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9832327365875244 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[215/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1038
log--[215/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1441
Epoch 0 Test (client-0):        Loss 0.6729 (0.6729)    Prec@1 84.375 (84.375)
Epoch 50        Test (client-0):        Loss 0.6077 (0.6176)    Prec@1 85.938 (84.972)
 * Prec@1 84.940
train_one_ep_time:11.539012908935547 s
feature_infer_one_ep_time:3.043426752090454 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6847991943359375 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[216/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1890
log--[216/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1452
Epoch 0 Test (client-0):        Loss 0.6882 (0.6882)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.6328 (0.6209)    Prec@1 83.594 (84.926)
 * Prec@1 84.870
train_one_ep_time:13.302114009857178 s
feature_infer_one_ep_time:2.8887336254119873 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7805383205413818 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[217/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1193
log--[217/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1428
Epoch 0 Test (client-0):        Loss 0.6928 (0.6928)    Prec@1 86.719 (86.719)
Epoch 50        Test (client-0):        Loss 0.6333 (0.6239)    Prec@1 82.031 (85.080)
 * Prec@1 84.990
train_one_ep_time:11.668221235275269 s
feature_infer_one_ep_time:3.0023491382598877 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6583402156829834 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[218/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1076
log--[218/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1446
Epoch 0 Test (client-0):        Loss 0.6798 (0.6798)    Prec@1 86.719 (86.719)
Epoch 50        Test (client-0):        Loss 0.6239 (0.6322)    Prec@1 85.938 (85.126)
 * Prec@1 85.100
train_one_ep_time:13.425524473190308 s
feature_infer_one_ep_time:2.8927319049835205 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7405846118927002 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[219/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1735
log--[219/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1416
Epoch 0 Test (client-0):        Loss 0.7000 (0.7000)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.6261 (0.6303)    Prec@1 84.375 (84.957)
 * Prec@1 84.920
train_one_ep_time:11.569122552871704 s
feature_infer_one_ep_time:2.9943864345550537 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.661860466003418 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[220/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1458
log--[220/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1406
Epoch 0 Test (client-0):        Loss 0.6709 (0.6709)    Prec@1 87.500 (87.500)
Epoch 50        Test (client-0):        Loss 0.6145 (0.6384)    Prec@1 85.156 (84.911)
 * Prec@1 84.810
train_one_ep_time:11.86538052558899 s
feature_infer_one_ep_time:3.140183210372925 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7820825576782227 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[221/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1506
log--[221/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1389
Epoch 0 Test (client-0):        Loss 0.6310 (0.6310)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.6370 (0.6315)    Prec@1 84.375 (85.095)
 * Prec@1 85.020
train_one_ep_time:11.995126724243164 s
feature_infer_one_ep_time:5.575266599655151 s
torch.Size([50048, 8, 8, 8])
the mean of mutal infor is:(-8.478), the est mean of mutal infor is:(-4.841)
feature_clst_one_ep_time:0.7817261219024658 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[222/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1444
log--[222/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1411
Epoch 0 Test (client-0):        Loss 0.6711 (0.6711)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.6618 (0.6339)    Prec@1 83.594 (84.758)
 * Prec@1 84.830
train_one_ep_time:12.105344533920288 s
feature_infer_one_ep_time:3.1141176223754883 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8145294189453125 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[223/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1413
log--[223/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1364
Epoch 0 Test (client-0):        Loss 0.6852 (0.6852)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.6775 (0.6384)    Prec@1 82.031 (84.957)
 * Prec@1 84.860
train_one_ep_time:12.014832735061646 s
feature_infer_one_ep_time:6.083453416824341 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.140296459197998 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[224/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1474
log--[224/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1376
Epoch 0 Test (client-0):        Loss 0.7075 (0.7075)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.6663 (0.6370)    Prec@1 84.375 (85.018)
 * Prec@1 84.930
train_one_ep_time:12.176586627960205 s
feature_infer_one_ep_time:3.0764551162719727 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7124323844909668 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[225/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1022
log--[225/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1378
Epoch 0 Test (client-0):        Loss 0.7218 (0.7218)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.7068 (0.6441)    Prec@1 81.250 (84.620)
 * Prec@1 84.690
train_one_ep_time:14.031960487365723 s
feature_infer_one_ep_time:3.3073413372039795 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6723566055297852 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[226/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1538
log--[226/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1375
Epoch 0 Test (client-0):        Loss 0.6946 (0.6946)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.6831 (0.6452)    Prec@1 82.031 (84.942)
 * Prec@1 84.830
train_one_ep_time:11.972687244415283 s
feature_infer_one_ep_time:3.8563661575317383 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9036448001861572 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[227/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1611
log--[227/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1357
Epoch 0 Test (client-0):        Loss 0.7184 (0.7184)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.6912 (0.6519)    Prec@1 82.812 (84.605)
 * Prec@1 84.650
train_one_ep_time:13.418791770935059 s
feature_infer_one_ep_time:3.0049898624420166 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.7125122547149658 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[228/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1151
log--[228/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1359
Epoch 0 Test (client-0):        Loss 0.7622 (0.7622)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.6541 (0.6507)    Prec@1 82.031 (84.743)
 * Prec@1 84.800
train_one_ep_time:11.336911678314209 s
feature_infer_one_ep_time:2.8547282218933105 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6221222877502441 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[229/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1786
log--[229/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1343
Epoch 0 Test (client-0):        Loss 0.7092 (0.7092)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.7357 (0.6497)    Prec@1 80.469 (84.835)
 * Prec@1 84.820
train_one_ep_time:11.512108564376831 s
feature_infer_one_ep_time:2.8118083477020264 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.5759515762329102 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[230/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1325
log--[230/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1354
Epoch 0 Test (client-0):        Loss 0.7145 (0.7145)    Prec@1 85.938 (85.938)
Epoch 50        Test (client-0):        Loss 0.7334 (0.6484)    Prec@1 81.250 (85.064)
 * Prec@1 84.990
train_one_ep_time:11.815513610839844 s
feature_infer_one_ep_time:5.609557628631592 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:1.0186402797698975 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[231/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1800
log--[231/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1310
Epoch 0 Test (client-0):        Loss 0.6984 (0.6984)    Prec@1 88.281 (88.281)
Epoch 50        Test (client-0):        Loss 0.6944 (0.6415)    Prec@1 82.031 (84.926)
 * Prec@1 84.850
train_one_ep_time:11.641435861587524 s
feature_infer_one_ep_time:2.9131312370300293 s
torch.Size([50048, 8, 8, 8])
the mean of mutal infor is:(-8.483), the est mean of mutal infor is:(-4.852)
feature_clst_one_ep_time:0.6822495460510254 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[232/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1192
log--[232/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1319
Epoch 0 Test (client-0):        Loss 0.7035 (0.7035)    Prec@1 87.500 (87.500)
Epoch 50        Test (client-0):        Loss 0.6911 (0.6505)    Prec@1 82.812 (85.095)
 * Prec@1 84.970
train_one_ep_time:11.05081033706665 s
feature_infer_one_ep_time:4.5558531284332275 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.9146454334259033 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[233/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.0750
log--[233/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1300
Epoch 0 Test (client-0):        Loss 0.7159 (0.7159)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.6744 (0.6516)    Prec@1 81.250 (84.651)
 * Prec@1 84.830
train_one_ep_time:11.463227987289429 s
feature_infer_one_ep_time:2.927635431289673 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.4549722671508789 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[234/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1687
log--[234/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1318
Epoch 0 Test (client-0):        Loss 0.6891 (0.6891)    Prec@1 85.156 (85.156)
Epoch 50        Test (client-0):        Loss 0.7376 (0.6579)    Prec@1 82.812 (85.034)
 * Prec@1 84.970
train_one_ep_time:11.412664413452148 s
feature_infer_one_ep_time:4.867863416671753 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6282811164855957 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[235/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1170
log--[235/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1326
Epoch 0 Test (client-0):        Loss 0.7398 (0.7398)    Prec@1 84.375 (84.375)
Epoch 50        Test (client-0):        Loss 0.7005 (0.6568)    Prec@1 81.250 (84.865)
 * Prec@1 84.860
train_one_ep_time:11.671751022338867 s
feature_infer_one_ep_time:2.9991564750671387 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.8242201805114746 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[236/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1400
log--[236/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1290
Epoch 0 Test (client-0):        Loss 0.7715 (0.7715)    Prec@1 83.594 (83.594)
Epoch 50        Test (client-0):        Loss 0.6794 (0.6628)    Prec@1 81.250 (84.482)
 * Prec@1 84.560
train_one_ep_time:11.298970222473145 s
feature_infer_one_ep_time:5.093137502670288 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6482300758361816 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[237/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.0963
log--[237/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1335
Epoch 0 Test (client-0):        Loss 0.7084 (0.7084)    Prec@1 84.375 (84.375)
Epoch 50        Test (client-0):        Loss 0.6697 (0.6541)    Prec@1 82.812 (84.758)
 * Prec@1 84.720
train_one_ep_time:11.524407863616943 s
feature_infer_one_ep_time:2.804929256439209 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.5886404514312744 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[238/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1915
log--[238/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1299
Epoch 0 Test (client-0):        Loss 0.7830 (0.7830)    Prec@1 82.812 (82.812)
Epoch 50        Test (client-0):        Loss 0.7122 (0.6654)    Prec@1 82.031 (84.635)
 * Prec@1 84.560
train_one_ep_time:11.204984188079834 s
feature_infer_one_ep_time:3.0160722732543945 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6861865520477295 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[239/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.0947
log--[239/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1261
Epoch 0 Test (client-0):        Loss 0.7382 (0.7382)    Prec@1 84.375 (84.375)
Epoch 50        Test (client-0):        Loss 0.6507 (0.6502)    Prec@1 81.250 (84.666)
 * Prec@1 84.670
train_one_ep_time:13.383322477340698 s
feature_infer_one_ep_time:2.7188992500305176 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6598026752471924 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Train in V2_epoch style
log--[240/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 0.0614
log--[240/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 0.1303
Epoch 0 Test (client-0):        Loss 0.7178 (0.7178)    Prec@1 86.719 (86.719)
Epoch 50        Test (client-0):        Loss 0.6431 (0.6581)    Prec@1 80.469 (84.743)
 * Prec@1 84.740
train_one_ep_time:10.82332730293274 s
feature_infer_one_ep_time:2.9044342041015625 s
torch.Size([50048, 8, 8, 8])
feature_clst_one_ep_time:0.6810388565063477 s
lambd value is: 0.0 learning rate is: 8.000000000000002e-05
Best Average Validation Accuracy is 85.13
2025-06-19 01:08:26.147212: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-19 01:08:26.326534: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Split Learning Scheme: Overall Cutting_layer 4/13
Total number of batches per epoch for each client is  391
32
original channel size of smashed-data is 128
added bottleneck, new channel size of smashed-data is 8
local:
Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): LCALayer()
  (10): Sigmoid()
)
cloud:
Sequential(
  (0): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (10): ReLU(inplace=True)
  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (13): ReLU(inplace=True)
  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (17): ReLU(inplace=True)
  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (20): ReLU(inplace=True)
  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
classifier:
Sequential(
  (0): Dropout(p=0.5, inplace=False)
  (1): Linear(in_features=512, out_features=512, bias=True)
  (2): ReLU(inplace=True)
  (3): Dropout(p=0.5, inplace=False)
  (4): Linear(in_features=512, out_features=512, bias=True)
  (5): ReLU(inplace=True)
  (6): Linear(in_features=512, out_features=10, bias=True)
)
the test model is: best
load client 0's local
./saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125/pretrain_False_lambd_0_noise_0.01_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125/checkpoint_f_best.tar
load cloud
load classifier
Traceback (most recent call last):
  File "/home/asher/CEM/main_test_MIA.py", line 178, in <module>
    images = torch.load("./test_cifar10_image.pt")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/asher/anaconda3/lib/python3.12/site-packages/torch/serialization.py", line 1479, in load
    with _open_file_like(f, "rb") as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/asher/anaconda3/lib/python3.12/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/asher/anaconda3/lib/python3.12/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './test_cifar10_image.pt'
2025-06-19 01:08:39.588331: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-19 01:08:39.608474: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Split Learning Scheme: Overall Cutting_layer 4/13
Total number of batches per epoch for each client is  391
32
original channel size of smashed-data is 128
added bottleneck, new channel size of smashed-data is 8
local:
Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): LCALayer()
  (10): Sigmoid()
)
cloud:
Sequential(
  (0): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (10): ReLU(inplace=True)
  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (13): ReLU(inplace=True)
  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (17): ReLU(inplace=True)
  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (20): ReLU(inplace=True)
  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
classifier:
Sequential(
  (0): Dropout(p=0.5, inplace=False)
  (1): Linear(in_features=512, out_features=512, bias=True)
  (2): ReLU(inplace=True)
  (3): Dropout(p=0.5, inplace=False)
  (4): Linear(in_features=512, out_features=512, bias=True)
  (5): ReLU(inplace=True)
  (6): Linear(in_features=512, out_features=10, bias=True)
)
Namespace(arch='vgg11_bn_sgm', cutlayer=4, batch_size=128, filename='pretrain_False_lambd_0_noise_0.025_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125', folder='saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125', num_client=1, num_epochs=240, learning_rate=0.05, lambd=0.0, dataset_portion=1.0, client_sample_ratio=1.0, noniid=1.0, local_lr=-1.0, dataset='cifar10', scheme='V2_epoch', regularization='Gaussian_kl', regularization_strength=0.025, var_threshold=0.125, AT_regularization='SCA_new', AT_regularization_strength=0.3, log_entropy=1.0, ssim_threshold=0.5, gan_AE_type='res_normN4C64', gan_loss_type='SSIM', bottleneck_option='noRELU_C8S1', optimize_computation=1, decoder_sync=False, load_from_checkpoint=False, load_from_checkpoint_server=False, transfer_source_task='cifar100', finetune_freeze_bn=False, save_more_checkpoints=False, initialize_different=False, random_seed=125)
Model's smashed-data size is torch.Size([1, 8, 8, 8])
Real Train Phase: done by all clients, for total 240 epochs
GAN training interval N (once every N step) is set to 1!
Train in V2_epoch style
/home/asher/CEM/model_training_paral_pruning.py:1720: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_loss_list.append(torch.tensor(train_loss))
log--[1/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3266
log--[1/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3213
Epoch 0 Test (client-0):        Loss 2.3120 (2.3120)    Prec@1 8.594 (8.594)
Epoch 50        Test (client-0):        Loss 2.3034 (2.3040)    Prec@1 17.188 (11.397)
 * Prec@1 11.170
best model saved at: 1
train_one_ep_time:13.69326663017273 s
feature_infer_one_ep_time:2.5120959281921387 s
torch.Size([50048, 8, 8, 8])
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
/home/asher/CEM/model_training_paral_pruning.py:1909: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  rob_list.append(torch.tensor(log_det_mean.detach().cpu()))
the mean of mutal infor is:(-5.030), the est mean of mutal infor is:(-3.462)
feature_clst_one_ep_time:1.6957478523254395 s
/home/asher/anaconda3/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.
  warnings.warn(
QObject::moveToThread: Current thread (0x52d884f0) is not the object's thread (0x77718b30).
Cannot move to target thread (0x52d884f0)

qt.qpa.plugin: Could not load the Qt platform plugin "xcb" in "/home/asher/anaconda3/lib/python3.12/site-packages/cv2/qt/plugins" even though it was found.
This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

Available platform plugins are: xcb, eglfs, minimal, minimalegl, offscreen, vnc, webgl.

run_exp.sh: line 37: 294015 Aborted                 (core dumped) CUDA_VISIBLE_DEVICES=${GPU_id} python main_MIA.py --arch=${arch} --cutlayer=$cutlayer --batch_size=${batch_size} --filename=$filename --num_client=$num_client --num_epochs=$num_epochs --dataset=$dataset --scheme=$scheme --regularization=${regularization} --regularization_strength=${regularization_strength} --log_entropy=${log_entropy} --AT_regularization=${AT_regularization} --AT_regularization_strength=${AT_regularization_strength} --random_seed=$random_seed --learning_rate=$learning_rate --lambd=$lambd --gan_AE_type ${train_gan_AE_type} --gan_loss_type ${gan_loss_type} --local_lr $local_lr --bottleneck_option ${bottleneck_option} --folder ${folder_name} --ssim_threshold ${ssim_threshold} --var_threshold ${var_threshold}
2025-06-19 01:10:53.734328: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-19 01:10:53.760897: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Split Learning Scheme: Overall Cutting_layer 4/13
Total number of batches per epoch for each client is  391
32
original channel size of smashed-data is 128
added bottleneck, new channel size of smashed-data is 8
local:
Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): LCALayer()
  (10): Sigmoid()
)
cloud:
Sequential(
  (0): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (10): ReLU(inplace=True)
  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (13): ReLU(inplace=True)
  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (17): ReLU(inplace=True)
  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (20): ReLU(inplace=True)
  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
classifier:
Sequential(
  (0): Dropout(p=0.5, inplace=False)
  (1): Linear(in_features=512, out_features=512, bias=True)
  (2): ReLU(inplace=True)
  (3): Dropout(p=0.5, inplace=False)
  (4): Linear(in_features=512, out_features=512, bias=True)
  (5): ReLU(inplace=True)
  (6): Linear(in_features=512, out_features=10, bias=True)
)
the test model is: best
load client 0's local
./saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125/pretrain_False_lambd_0_noise_0.025_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125/checkpoint_f_best.tar
load cloud
load classifier
Traceback (most recent call last):
  File "/home/asher/CEM/main_test_MIA.py", line 178, in <module>
    images = torch.load("./test_cifar10_image.pt")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/asher/anaconda3/lib/python3.12/site-packages/torch/serialization.py", line 1479, in load
    with _open_file_like(f, "rb") as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/asher/anaconda3/lib/python3.12/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/asher/anaconda3/lib/python3.12/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './test_cifar10_image.pt'
2025-06-19 01:11:03.238467: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-19 01:11:03.259414: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Split Learning Scheme: Overall Cutting_layer 4/13
Total number of batches per epoch for each client is  391
32
original channel size of smashed-data is 128
added bottleneck, new channel size of smashed-data is 8
local:
Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): LCALayer()
  (10): Sigmoid()
)
cloud:
Sequential(
  (0): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (10): ReLU(inplace=True)
  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (13): ReLU(inplace=True)
  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (17): ReLU(inplace=True)
  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (20): ReLU(inplace=True)
  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
classifier:
Sequential(
  (0): Dropout(p=0.5, inplace=False)
  (1): Linear(in_features=512, out_features=512, bias=True)
  (2): ReLU(inplace=True)
  (3): Dropout(p=0.5, inplace=False)
  (4): Linear(in_features=512, out_features=512, bias=True)
  (5): ReLU(inplace=True)
  (6): Linear(in_features=512, out_features=10, bias=True)
)
Namespace(arch='vgg11_bn_sgm', cutlayer=4, batch_size=128, filename='pretrain_False_lambd_0_noise_0.05_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125', folder='saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125', num_client=1, num_epochs=240, learning_rate=0.05, lambd=0.0, dataset_portion=1.0, client_sample_ratio=1.0, noniid=1.0, local_lr=-1.0, dataset='cifar10', scheme='V2_epoch', regularization='Gaussian_kl', regularization_strength=0.05, var_threshold=0.125, AT_regularization='SCA_new', AT_regularization_strength=0.3, log_entropy=1.0, ssim_threshold=0.5, gan_AE_type='res_normN4C64', gan_loss_type='SSIM', bottleneck_option='noRELU_C8S1', optimize_computation=1, decoder_sync=False, load_from_checkpoint=False, load_from_checkpoint_server=False, transfer_source_task='cifar100', finetune_freeze_bn=False, save_more_checkpoints=False, initialize_different=False, random_seed=125)
Model's smashed-data size is torch.Size([1, 8, 8, 8])
Real Train Phase: done by all clients, for total 240 epochs
GAN training interval N (once every N step) is set to 1!
Train in V2_epoch style
/home/asher/CEM/model_training_paral_pruning.py:1720: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_loss_list.append(torch.tensor(train_loss))
log--[1/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3266
log--[1/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3214
Epoch 0 Test (client-0):        Loss 2.3125 (2.3125)    Prec@1 9.375 (9.375)
Epoch 50        Test (client-0):        Loss 2.3040 (2.3040)    Prec@1 17.188 (11.244)
 * Prec@1 11.040
best model saved at: 1
train_one_ep_time:10.733920097351074 s
feature_infer_one_ep_time:2.541368246078491 s
torch.Size([50048, 8, 8, 8])
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
/home/asher/CEM/model_training_paral_pruning.py:1909: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  rob_list.append(torch.tensor(log_det_mean.detach().cpu()))
the mean of mutal infor is:(-4.588), the est mean of mutal infor is:(-3.392)
feature_clst_one_ep_time:1.1332213878631592 s
/home/asher/anaconda3/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.
  warnings.warn(
QObject::moveToThread: Current thread (0x5d1e3c80) is not the object's thread (0x819cff40).
Cannot move to target thread (0x5d1e3c80)

qt.qpa.plugin: Could not load the Qt platform plugin "xcb" in "/home/asher/anaconda3/lib/python3.12/site-packages/cv2/qt/plugins" even though it was found.
This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

Available platform plugins are: xcb, eglfs, minimal, minimalegl, offscreen, vnc, webgl.

run_exp.sh: line 37: 295160 Aborted                 (core dumped) CUDA_VISIBLE_DEVICES=${GPU_id} python main_MIA.py --arch=${arch} --cutlayer=$cutlayer --batch_size=${batch_size} --filename=$filename --num_client=$num_client --num_epochs=$num_epochs --dataset=$dataset --scheme=$scheme --regularization=${regularization} --regularization_strength=${regularization_strength} --log_entropy=${log_entropy} --AT_regularization=${AT_regularization} --AT_regularization_strength=${AT_regularization_strength} --random_seed=$random_seed --learning_rate=$learning_rate --lambd=$lambd --gan_AE_type ${train_gan_AE_type} --gan_loss_type ${gan_loss_type} --local_lr $local_lr --bottleneck_option ${bottleneck_option} --folder ${folder_name} --ssim_threshold ${ssim_threshold} --var_threshold ${var_threshold}
2025-06-19 01:13:19.457277: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-19 01:13:19.478183: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Split Learning Scheme: Overall Cutting_layer 4/13
Total number of batches per epoch for each client is  391
32
original channel size of smashed-data is 128
added bottleneck, new channel size of smashed-data is 8
local:
Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): LCALayer()
  (10): Sigmoid()
)
cloud:
Sequential(
  (0): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (10): ReLU(inplace=True)
  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (13): ReLU(inplace=True)
  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (17): ReLU(inplace=True)
  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (20): ReLU(inplace=True)
  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
classifier:
Sequential(
  (0): Dropout(p=0.5, inplace=False)
  (1): Linear(in_features=512, out_features=512, bias=True)
  (2): ReLU(inplace=True)
  (3): Dropout(p=0.5, inplace=False)
  (4): Linear(in_features=512, out_features=512, bias=True)
  (5): ReLU(inplace=True)
  (6): Linear(in_features=512, out_features=10, bias=True)
)
the test model is: best
load client 0's local
./saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125/pretrain_False_lambd_0_noise_0.05_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125/checkpoint_f_best.tar
load cloud
load classifier
Traceback (most recent call last):
  File "/home/asher/CEM/main_test_MIA.py", line 178, in <module>
    images = torch.load("./test_cifar10_image.pt")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/asher/anaconda3/lib/python3.12/site-packages/torch/serialization.py", line 1479, in load
    with _open_file_like(f, "rb") as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/asher/anaconda3/lib/python3.12/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/asher/anaconda3/lib/python3.12/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './test_cifar10_image.pt'
2025-06-19 01:13:28.357971: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-19 01:13:28.378398: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Split Learning Scheme: Overall Cutting_layer 4/13
Total number of batches per epoch for each client is  391
32
original channel size of smashed-data is 128
added bottleneck, new channel size of smashed-data is 8
local:
Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): LCALayer()
  (10): Sigmoid()
)
cloud:
Sequential(
  (0): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (10): ReLU(inplace=True)
  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (13): ReLU(inplace=True)
  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (17): ReLU(inplace=True)
  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (20): ReLU(inplace=True)
  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
classifier:
Sequential(
  (0): Dropout(p=0.5, inplace=False)
  (1): Linear(in_features=512, out_features=512, bias=True)
  (2): ReLU(inplace=True)
  (3): Dropout(p=0.5, inplace=False)
  (4): Linear(in_features=512, out_features=512, bias=True)
  (5): ReLU(inplace=True)
  (6): Linear(in_features=512, out_features=10, bias=True)
)
Namespace(arch='vgg11_bn_sgm', cutlayer=4, batch_size=128, filename='pretrain_False_lambd_0_noise_0.1_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125', folder='saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125', num_client=1, num_epochs=240, learning_rate=0.05, lambd=0.0, dataset_portion=1.0, client_sample_ratio=1.0, noniid=1.0, local_lr=-1.0, dataset='cifar10', scheme='V2_epoch', regularization='Gaussian_kl', regularization_strength=0.1, var_threshold=0.125, AT_regularization='SCA_new', AT_regularization_strength=0.3, log_entropy=1.0, ssim_threshold=0.5, gan_AE_type='res_normN4C64', gan_loss_type='SSIM', bottleneck_option='noRELU_C8S1', optimize_computation=1, decoder_sync=False, load_from_checkpoint=False, load_from_checkpoint_server=False, transfer_source_task='cifar100', finetune_freeze_bn=False, save_more_checkpoints=False, initialize_different=False, random_seed=125)
Model's smashed-data size is torch.Size([1, 8, 8, 8])
Real Train Phase: done by all clients, for total 240 epochs
GAN training interval N (once every N step) is set to 1!
Train in V2_epoch style
/home/asher/CEM/model_training_paral_pruning.py:1720: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_loss_list.append(torch.tensor(train_loss))
log--[1/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3291
log--[1/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3217
Epoch 0 Test (client-0):        Loss 2.3143 (2.3143)    Prec@1 9.375 (9.375)
Epoch 50        Test (client-0):        Loss 2.3061 (2.3044)    Prec@1 14.062 (11.014)
 * Prec@1 10.720
best model saved at: 1
train_one_ep_time:12.932140350341797 s
feature_infer_one_ep_time:2.5000383853912354 s
torch.Size([50048, 8, 8, 8])
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
/home/asher/CEM/model_training_paral_pruning.py:1909: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  rob_list.append(torch.tensor(log_det_mean.detach().cpu()))
the mean of mutal infor is:(-3.862), the est mean of mutal infor is:(-3.170)
feature_clst_one_ep_time:1.1108629703521729 s
/home/asher/anaconda3/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.
  warnings.warn(
QObject::moveToThread: Current thread (0x6045dba0) is not the object's thread (0x68fcd760).
Cannot move to target thread (0x6045dba0)

qt.qpa.plugin: Could not load the Qt platform plugin "xcb" in "/home/asher/anaconda3/lib/python3.12/site-packages/cv2/qt/plugins" even though it was found.
This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

Available platform plugins are: xcb, eglfs, minimal, minimalegl, offscreen, vnc, webgl.

run_exp.sh: line 37: 296328 Aborted                 (core dumped) CUDA_VISIBLE_DEVICES=${GPU_id} python main_MIA.py --arch=${arch} --cutlayer=$cutlayer --batch_size=${batch_size} --filename=$filename --num_client=$num_client --num_epochs=$num_epochs --dataset=$dataset --scheme=$scheme --regularization=${regularization} --regularization_strength=${regularization_strength} --log_entropy=${log_entropy} --AT_regularization=${AT_regularization} --AT_regularization_strength=${AT_regularization_strength} --random_seed=$random_seed --learning_rate=$learning_rate --lambd=$lambd --gan_AE_type ${train_gan_AE_type} --gan_loss_type ${gan_loss_type} --local_lr $local_lr --bottleneck_option ${bottleneck_option} --folder ${folder_name} --ssim_threshold ${ssim_threshold} --var_threshold ${var_threshold}
2025-06-19 01:15:43.397412: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-19 01:15:43.419891: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Split Learning Scheme: Overall Cutting_layer 4/13
Total number of batches per epoch for each client is  391
32
original channel size of smashed-data is 128
added bottleneck, new channel size of smashed-data is 8
local:
Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): LCALayer()
  (10): Sigmoid()
)
cloud:
Sequential(
  (0): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (10): ReLU(inplace=True)
  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (13): ReLU(inplace=True)
  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (17): ReLU(inplace=True)
  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (20): ReLU(inplace=True)
  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
classifier:
Sequential(
  (0): Dropout(p=0.5, inplace=False)
  (1): Linear(in_features=512, out_features=512, bias=True)
  (2): ReLU(inplace=True)
  (3): Dropout(p=0.5, inplace=False)
  (4): Linear(in_features=512, out_features=512, bias=True)
  (5): ReLU(inplace=True)
  (6): Linear(in_features=512, out_features=10, bias=True)
)
the test model is: best
load client 0's local
./saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125/pretrain_False_lambd_0_noise_0.1_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125/checkpoint_f_best.tar
load cloud
load classifier
Traceback (most recent call last):
  File "/home/asher/CEM/main_test_MIA.py", line 178, in <module>
    images = torch.load("./test_cifar10_image.pt")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/asher/anaconda3/lib/python3.12/site-packages/torch/serialization.py", line 1479, in load
    with _open_file_like(f, "rb") as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/asher/anaconda3/lib/python3.12/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/asher/anaconda3/lib/python3.12/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './test_cifar10_image.pt'
2025-06-19 01:15:52.571989: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-19 01:15:52.593186: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Split Learning Scheme: Overall Cutting_layer 4/13
Total number of batches per epoch for each client is  391
32
original channel size of smashed-data is 128
added bottleneck, new channel size of smashed-data is 8
local:
Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): LCALayer()
  (10): Sigmoid()
)
cloud:
Sequential(
  (0): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (10): ReLU(inplace=True)
  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (13): ReLU(inplace=True)
  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (17): ReLU(inplace=True)
  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (20): ReLU(inplace=True)
  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
classifier:
Sequential(
  (0): Dropout(p=0.5, inplace=False)
  (1): Linear(in_features=512, out_features=512, bias=True)
  (2): ReLU(inplace=True)
  (3): Dropout(p=0.5, inplace=False)
  (4): Linear(in_features=512, out_features=512, bias=True)
  (5): ReLU(inplace=True)
  (6): Linear(in_features=512, out_features=10, bias=True)
)
Namespace(arch='vgg11_bn_sgm', cutlayer=4, batch_size=128, filename='pretrain_False_lambd_0_noise_0.15_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125', folder='saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125', num_client=1, num_epochs=240, learning_rate=0.05, lambd=0.0, dataset_portion=1.0, client_sample_ratio=1.0, noniid=1.0, local_lr=-1.0, dataset='cifar10', scheme='V2_epoch', regularization='Gaussian_kl', regularization_strength=0.15, var_threshold=0.125, AT_regularization='SCA_new', AT_regularization_strength=0.3, log_entropy=1.0, ssim_threshold=0.5, gan_AE_type='res_normN4C64', gan_loss_type='SSIM', bottleneck_option='noRELU_C8S1', optimize_computation=1, decoder_sync=False, load_from_checkpoint=False, load_from_checkpoint_server=False, transfer_source_task='cifar100', finetune_freeze_bn=False, save_more_checkpoints=False, initialize_different=False, random_seed=125)
Model's smashed-data size is torch.Size([1, 8, 8, 8])
Real Train Phase: done by all clients, for total 240 epochs
GAN training interval N (once every N step) is set to 1!
Train in V2_epoch style
/home/asher/CEM/model_training_paral_pruning.py:1720: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_loss_list.append(torch.tensor(train_loss))
log--[1/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3290
log--[1/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3219
Epoch 0 Test (client-0):        Loss 2.3177 (2.3177)    Prec@1 9.375 (9.375)
Epoch 50        Test (client-0):        Loss 2.3080 (2.3048)    Prec@1 11.719 (10.892)
 * Prec@1 10.680
best model saved at: 1
train_one_ep_time:10.871981382369995 s
feature_infer_one_ep_time:2.4711930751800537 s
torch.Size([50048, 8, 8, 8])
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
/home/asher/CEM/model_training_paral_pruning.py:1909: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  rob_list.append(torch.tensor(log_det_mean.detach().cpu()))
the mean of mutal infor is:(-3.313), the est mean of mutal infor is:(-2.896)
feature_clst_one_ep_time:1.0964632034301758 s
/home/asher/anaconda3/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.
  warnings.warn(
QObject::moveToThread: Current thread (0x8a7e47d0) is not the object's thread (0x8c2f2c80).
Cannot move to target thread (0x8a7e47d0)

qt.qpa.plugin: Could not load the Qt platform plugin "xcb" in "/home/asher/anaconda3/lib/python3.12/site-packages/cv2/qt/plugins" even though it was found.
This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

Available platform plugins are: xcb, eglfs, minimal, minimalegl, offscreen, vnc, webgl.

run_exp.sh: line 37: 297469 Aborted                 (core dumped) CUDA_VISIBLE_DEVICES=${GPU_id} python main_MIA.py --arch=${arch} --cutlayer=$cutlayer --batch_size=${batch_size} --filename=$filename --num_client=$num_client --num_epochs=$num_epochs --dataset=$dataset --scheme=$scheme --regularization=${regularization} --regularization_strength=${regularization_strength} --log_entropy=${log_entropy} --AT_regularization=${AT_regularization} --AT_regularization_strength=${AT_regularization_strength} --random_seed=$random_seed --learning_rate=$learning_rate --lambd=$lambd --gan_AE_type ${train_gan_AE_type} --gan_loss_type ${gan_loss_type} --local_lr $local_lr --bottleneck_option ${bottleneck_option} --folder ${folder_name} --ssim_threshold ${ssim_threshold} --var_threshold ${var_threshold}
2025-06-19 01:17:45.291865: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-19 01:17:45.315255: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Split Learning Scheme: Overall Cutting_layer 4/13
Total number of batches per epoch for each client is  391
32
original channel size of smashed-data is 128
added bottleneck, new channel size of smashed-data is 8
local:
Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): LCALayer()
  (10): Sigmoid()
)
cloud:
Sequential(
  (0): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (10): ReLU(inplace=True)
  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (13): ReLU(inplace=True)
  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (17): ReLU(inplace=True)
  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (20): ReLU(inplace=True)
  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
classifier:
Sequential(
  (0): Dropout(p=0.5, inplace=False)
  (1): Linear(in_features=512, out_features=512, bias=True)
  (2): ReLU(inplace=True)
  (3): Dropout(p=0.5, inplace=False)
  (4): Linear(in_features=512, out_features=512, bias=True)
  (5): ReLU(inplace=True)
  (6): Linear(in_features=512, out_features=10, bias=True)
)
the test model is: best
load client 0's local
./saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125/pretrain_False_lambd_0_noise_0.15_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125/checkpoint_f_best.tar
load cloud
load classifier
Traceback (most recent call last):
  File "/home/asher/CEM/main_test_MIA.py", line 178, in <module>
    images = torch.load("./test_cifar10_image.pt")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/asher/anaconda3/lib/python3.12/site-packages/torch/serialization.py", line 1479, in load
    with _open_file_like(f, "rb") as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/asher/anaconda3/lib/python3.12/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/asher/anaconda3/lib/python3.12/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './test_cifar10_image.pt'
2025-06-19 01:17:56.370218: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-19 01:17:56.392620: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Split Learning Scheme: Overall Cutting_layer 4/13
Total number of batches per epoch for each client is  391
32
original channel size of smashed-data is 128
added bottleneck, new channel size of smashed-data is 8
local:
Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): LCALayer()
  (10): Sigmoid()
)
cloud:
Sequential(
  (0): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (3): ReLU(inplace=True)
  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (6): ReLU(inplace=True)
  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (10): ReLU(inplace=True)
  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (13): ReLU(inplace=True)
  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (17): ReLU(inplace=True)
  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (20): ReLU(inplace=True)
  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
classifier:
Sequential(
  (0): Dropout(p=0.5, inplace=False)
  (1): Linear(in_features=512, out_features=512, bias=True)
  (2): ReLU(inplace=True)
  (3): Dropout(p=0.5, inplace=False)
  (4): Linear(in_features=512, out_features=512, bias=True)
  (5): ReLU(inplace=True)
  (6): Linear(in_features=512, out_features=10, bias=True)
)
Namespace(arch='vgg11_bn_sgm', cutlayer=4, batch_size=128, filename='pretrain_False_lambd_8_noise_0.01_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125', folder='saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125', num_client=1, num_epochs=240, learning_rate=0.05, lambd=8.0, dataset_portion=1.0, client_sample_ratio=1.0, noniid=1.0, local_lr=-1.0, dataset='cifar10', scheme='V2_epoch', regularization='Gaussian_kl', regularization_strength=0.01, var_threshold=0.125, AT_regularization='SCA_new', AT_regularization_strength=0.3, log_entropy=1.0, ssim_threshold=0.5, gan_AE_type='res_normN4C64', gan_loss_type='SSIM', bottleneck_option='noRELU_C8S1', optimize_computation=1, decoder_sync=False, load_from_checkpoint=False, load_from_checkpoint_server=False, transfer_source_task='cifar100', finetune_freeze_bn=False, save_more_checkpoints=False, initialize_different=False, random_seed=125)
Model's smashed-data size is torch.Size([1, 8, 8, 8])
Real Train Phase: done by all clients, for total 240 epochs
GAN training interval N (once every N step) is set to 1!
Train in V2_epoch style
/home/asher/CEM/model_training_paral_pruning.py:1720: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_loss_list.append(torch.tensor(train_loss))
log--[1/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3268
log--[1/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3213
Epoch 0 Test (client-0):        Loss 2.3128 (2.3128)    Prec@1 7.812 (7.812)
Epoch 50        Test (client-0):        Loss 2.3040 (2.3041)    Prec@1 17.188 (11.458)
 * Prec@1 11.190
best model saved at: 1
train_one_ep_time:11.039677143096924 s
feature_infer_one_ep_time:2.5546698570251465 s
torch.Size([50048, 8, 8, 8])
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
randomized selected centroids
/home/asher/CEM/model_training_paral_pruning.py:1909: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  rob_list.append(torch.tensor(log_det_mean.detach().cpu()))
the mean of mutal infor is:(-5.270), the est mean of mutal infor is:(-3.484)
feature_clst_one_ep_time:1.1864492893218994 s
/home/asher/anaconda3/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.
  warnings.warn(
QObject::moveToThread: Current thread (0x43f1a830) is not the object's thread (0x84d8b470).
Cannot move to target thread (0x43f1a830)

qt.qpa.plugin: Could not load the Qt platform plugin "xcb" in "/home/asher/anaconda3/lib/python3.12/site-packages/cv2/qt/plugins" even though it was found.
This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

Available platform plugins are: xcb, eglfs, minimal, minimalegl, offscreen, vnc, webgl.